<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>数据仓库架构简单介绍</title>
      <link href="/2019/08/28/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/"/>
      <url>/2019/08/28/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<center>针对数据仓库架构的相关概念进行简单的解释说明，对数据仓库的建模方式进行简单的描述。</center><a id="more"></a><p><img src="//heltman.github.io/2019/08/28/数据仓库架构/%E5%A4%A7%E6%95%B0%E6%8D%AE.jpg" alt="大数据"></p><h2 id="数据仓库概念"><a href="#数据仓库概念" class="headerlink" title="数据仓库概念"></a>数据仓库概念</h2><h3 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h3><h4 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h4><ul><li>需求的变化</li><li>业务系统的建设逐渐完善</li><li>分析类需求不断增加</li><li>不断增加的信息孤岛导致数据集成问题不断增加</li></ul><h4 id="技术发展状况"><a href="#技术发展状况" class="headerlink" title="技术发展状况"></a>技术发展状况</h4><ul><li>关系数据库技术日趋成熟</li><li>报表和复杂查询处理起来非常困难</li><li>各个系统之间数据不一致</li></ul><p>常用关系型数据仓库：greenplum，vertica</p><h4 id="ODS：操作型数据仓库"><a href="#ODS：操作型数据仓库" class="headerlink" title="ODS：操作型数据仓库"></a>ODS：操作型数据仓库</h4><p>早期数据仓库模型，为企业提供即时的，操作型的，继承的数据集合，具有面向主题性，集成性，动态性，即时性，明细性等特性。面向即时的，一般是一小时，到一分钟。存储时间较短，7天5天等。基于战术的操作，和基于战略的数据仓库有所不同。</p><h4 id="STAGING-AREA（缓存区，分段存储区）"><a href="#STAGING-AREA（缓存区，分段存储区）" class="headerlink" title="STAGING AREA（缓存区，分段存储区）"></a>STAGING AREA（缓存区，分段存储区）</h4><p>为了保证数据移动的顺利进行而开设的阶段性数据存储空间，和ODS有重合，有ODS可以不要这个。如果数据仓库数据有问题可以直接从缓存区拿取数据，不需要从业务处拿取。</p><h4 id="DATA-MART（数据集市）"><a href="#DATA-MART（数据集市）" class="headerlink" title="DATA MART（数据集市）"></a>DATA MART（数据集市）</h4><p>数据集市是完整的数据仓库的一个逻辑子集，为了特定的应用目的或应用范围，而从数据仓库独立出来的一部分数据。，也可称为部门数据或主题数据。</p><h4 id="数据仓库设计"><a href="#数据仓库设计" class="headerlink" title="数据仓库设计"></a>数据仓库设计</h4><blockquote><p><strong>源系统 –&gt; ODS –&gt; DW –&gt; DM</strong></p></blockquote><p>建立数据仓库与数据集市，一般采用“自顶向下”和“自下而上”相结合的设计思想。</p><h4 id="OLAP（在线分析处理）"><a href="#OLAP（在线分析处理）" class="headerlink" title="OLAP（在线分析处理）"></a>OLAP（在线分析处理）</h4><p>OLAP是一种多为分析技术，用来满足决策用户在大量的业务数据中，从多角度探索业务活动的规律性、市场的运作趋势的分析需求，并辅助他们进行战略决策的制定。OLAP系统按照存储格式可以分为关系OLAP（ROLAP）、多维OLAP（MOLAP）和混合型OLAP（HOLAP）三种类型。Kylin属于多维OLAP</p><h4 id="OLAP-VA-OLTP（在线分析系统和在线交易系统）"><a href="#OLAP-VA-OLTP（在线分析系统和在线交易系统）" class="headerlink" title="OLAP VA OLTP（在线分析系统和在线交易系统）"></a>OLAP VA OLTP（在线分析系统和在线交易系统）</h4><table><thead><tr><th>ITEM</th><th>OLTP</th><th>OLAP</th></tr></thead><tbody><tr><td>用户</td><td>操作人员</td><td>分析决策人员，高级管理人员</td></tr><tr><td>功能</td><td>日常操作处理</td><td>分析决策</td></tr><tr><td>设计</td><td>面向应用</td><td>面向主题</td></tr><tr><td>数据</td><td>当前的，最新的细节的，二维的，分立的</td><td>历史的，聚集的，多维的，集成的，统一的</td></tr><tr><td>存取</td><td>读/写数十条记录</td><td>读上百万条记录</td></tr><tr><td>工作单位</td><td>简单的读写</td><td>复杂查询</td></tr><tr><td>DB大小</td><td>GB到TB</td><td>TB-PB级别</td></tr><tr><td>度量</td><td>事物吞吐量</td><td>查询吞吐量、响应时间</td></tr></tbody></table><hr><h2 id="数据仓库建模"><a href="#数据仓库建模" class="headerlink" title="数据仓库建模"></a>数据仓库建模</h2><p>范式建模、维度建模、Data Valut、Anchor</p><h4 id="范式建模：ER模型"><a href="#范式建模：ER模型" class="headerlink" title="范式建模：ER模型"></a>范式建模：ER模型</h4><p>自上而下（EDW-DM）的数据仓库架构。操作型或事务型系统的数据源，通过ETL抽取转换和加载到数据仓库的ODS层，然后通过ODS的数据建设原子数据的数据仓库EDW，EDW不是多维格式的，不方便上层应用做数据分析，所以需要通过汇总建设成多维格式的数据集市层。</p><p>优势：易于维护，高度集成</p><p>劣势：结构死板，部署周期较长</p><h3 id="维度建模"><a href="#维度建模" class="headerlink" title="维度建模"></a>维度建模</h3><p>以分析决策的需求出发构建模型，构建的数据模型为分析需求服务，因此它重点解决用户如何更快速完成分析需求，同时还有叫好的大规模复杂查询的响应性能。</p><p>维度模型是由一个规范化的事实表和反规范化的一些维度组成的</p><ul><li>一种非规范化的关系模型</li><li>表和表之间的关系通过关键字和外键来定义</li></ul><p>以良好的可理解性和方便的产生报表来进行数据组织，很少考虑修改的性能</p><p>通过SQL或者相关的工具实现数据的查询和维护</p><h4 id="维度表"><a href="#维度表" class="headerlink" title="维度表"></a>维度表</h4><p>每一张维度表对应现实世界中的一个对象或者概念</p><ul><li>例如：客户、产品、日期、地区、商场</li></ul><p>维度表的特征：</p><ul><li>包含了众多描述性的列：维度表的范围很宽（具有多个属性）</li><li>通常情况下，跟事实表相比，行数相对较小：通常&lt;10万条</li><li>内容相对固定：几乎就是一个类查找表或者编码表</li></ul><h4 id="事实表"><a href="#事实表" class="headerlink" title="事实表"></a>事实表</h4><p>每一个事实表通常包含了处理所关心的度量值</p><p>每一个事实表的行包括</p><ul><li>具有可加性的数值型的度量值</li><li>与维度表相连接的外键<ul><li>通常具有两个和两个以上的外键</li><li>外键之间表示维度表之间多对多的关系</li></ul></li></ul><p>事实表的特征</p><ul><li>非常大：包含几万、几十万甚至千万的记录</li><li>内容相对的窄，列数较少</li><li>经常发生变化</li></ul><h4 id="维度模型设计过程"><a href="#维度模型设计过程" class="headerlink" title="维度模型设计过程"></a>维度模型设计过程</h4><ol><li>选择业务过程</li><li>声明粒度</li><li>确定维度</li><li>确定事实</li></ol><hr><h2 id="数据治理"><a href="#数据治理" class="headerlink" title="数据治理"></a>数据治理</h2><h4 id="元数据管理"><a href="#元数据管理" class="headerlink" title="元数据管理"></a>元数据管理</h4><ul><li>技术元数据<ul><li>表级血缘分析</li><li>字段级别血缘分析</li><li>影响分析</li><li>数据审计</li></ul></li><li>业务元数据</li></ul><h4 id="主数据管理"><a href="#主数据管理" class="headerlink" title="主数据管理"></a>主数据管理</h4><hr><h2 id="数据仓库架构"><a href="#数据仓库架构" class="headerlink" title="数据仓库架构"></a>数据仓库架构</h2><p><img src="//heltman.github.io/2019/08/28/数据仓库架构/1558625207687.png" alt="1558625207687"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 入门文档 </tag>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark简单学习说明</title>
      <link href="/2019/08/28/Spark%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/08/28/Spark%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<center>Spark简单解释说明，待更新……</center><a id="more"></a><h2 id="配置Spark"><a href="#配置Spark" class="headerlink" title="配置Spark"></a>配置Spark</h2><p>连接Spark到任意hadoop集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### in conf/spark-env.sh ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If 'hadoop' binary is on your PATH</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(hadoop classpath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># With explicit path to 'hadoop' binary</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/path/to/hadoop/bin/hadoop classpath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Passing a Hadoop configuration directory</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(hadoop --config /path/to/configs classpath)</span><br></pre></td></tr></table></figure><h2 id="写Spark程序"><a href="#写Spark程序" class="headerlink" title="写Spark程序"></a>写Spark程序</h2><h3 id="与Spark连接"><a href="#与Spark连接" class="headerlink" title="与Spark连接"></a>与Spark连接</h3><p>Spark 1.6.1使用Scala 2.10。要在Scala中编写应用程序，您需要使用兼容的Scala版本（例如2.10.X）。</p><h4 id="编写Spark程序所需mvn依赖"><a href="#编写Spark程序所需mvn依赖" class="headerlink" title="编写Spark程序所需mvn依赖"></a>编写Spark程序所需mvn依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果需要连接HDFS，还需要如下依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>最后需要在程序中import一些Spark类</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br></pre></td></tr></table></figure><p>（在Spark 1.3.0之前，您需要显式导入org.apache.spark.SparkContext._以启用必要的隐式转换。)</p><h3 id="初始化Spark"><a href="#初始化Spark" class="headerlink" title="初始化Spark"></a>初始化Spark</h3><p>Spark程序必须做的第一件事是创建一个SparkContext对象，它告诉Spark如何访问集群。要创建SparkContext，首先需要构建一个包含有关应用程序信息的SparkConf对象。</p><p>每个JVM只能激活一个SparkContext。在创建新的SparkContext之前，必须先<code>stop()</code>。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(appName).setMaster(master)</span><br><span class="line"><span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure><p>appName参数是应用程序在集群UI上显示的名称。 master是Spark，Mesos或YARN群集URL，或者是以本地模式运行的特殊“本地”字符串。 实际上，在群集上运行时，您不希望在程序中对master进行硬编码，而是使用spark-submit启动应用程序并在那里接收它。 但是，对于本地测试和单元测试，您可以传递“local”以在进程中运行Spark。</p><h4 id="使用shell"><a href="#使用shell" class="headerlink" title="使用shell"></a>使用shell</h4><p>在Spark shell中，已经在名为<code>sc</code>的变量中为您创建了一个特殊的SparkContext 。制作自己的SparkContext将无法正常工作。您可以使用<code>--master</code>参数设置上下文连接到的主服务器，并且可以通过将逗号分隔的列表传递给参数来将JAR添加到类路径中<code>--jars</code>。您还可以通过为参数提供以逗号分隔的maven坐标列表，将依赖项（例如Spark包）添加到shell会话中<code>--packages</code>。任何可能存在依赖关系的其他存储库（例如SonaType）都可以传递给<code>--repositories</code>参数。例如，要<code>bin/spark-shell</code>在四个核心上运行，请使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-shell --master <span class="built_in">local</span>[4]</span><br></pre></td></tr></table></figure><p>或者，要将code.jar添加到其类路径中，请使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-shell --master <span class="built_in">local</span>[4] --jars code.jar</span><br></pre></td></tr></table></figure><p>要使用maven坐标包含依赖项:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-shell --master <span class="built_in">local</span>[4] --packages <span class="string">"org.example:example:0.1"</span></span><br></pre></td></tr></table></figure><p>有关选项的完整列表，请运行<code>spark-shell --help</code>。在幕后， <code>spark-shell</code>调用更通用的<span class="exturl" data-url="aHR0cDovL3NwYXJrLmFwYWNoZS5vcmcvZG9jcy8xLjYuMS9zdWJtaXR0aW5nLWFwcGxpY2F0aW9ucy5odG1s" title="http://spark.apache.org/docs/1.6.1/submitting-applications.html"><code>spark-submit</code>脚本<i class="fa fa-external-link"></i></span>。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 入门文档 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
