<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Zeppelin问题处理</title>
      <link href="/2019/09/05/Zeppelin%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/"/>
      <url>/2019/09/05/Zeppelin%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<center>Zeppelin常见问题处理方案，方便查询。</center><a id="more"></a><h1 id="Zeppelin配置"><a href="#Zeppelin配置" class="headerlink" title="Zeppelin配置"></a>Zeppelin配置</h1><h2 id="Zeppelin同时配置Spark和Spark2"><a href="#Zeppelin同时配置Spark和Spark2" class="headerlink" title="Zeppelin同时配置Spark和Spark2"></a>Zeppelin同时配置Spark和Spark2</h2><blockquote><p>起因：准备构建全新大数据平台，因此顺便研究一下将Spark1升级到Spark2，同时也就研究一下Zeppelin来可视化分析。现有平台是<code>HDP2.6.5</code>，自带<code>Spark1.6.3</code>、<code>Spark2.3.0</code>和<code>Zeppelin0.7.3</code>，使用Ambari轻松装好。</p><p><strong>可是问题来了，Zeppelin中Spark处理引擎都不能使用？？？</strong></p><p>难道打开方式不对？去Hortonworks官方文档里面看了一下，根本没提这回事，只说了同时安装Spark1和2时如何切换版本，大概这部分没有测试？</p></blockquote><h3 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h3><p>同时安装Spark1和2时，Zeppelin的Spark执行引擎都不能用，Web上执行后直接报错，报错大多为空指针异常、连接拒绝以及方法或类找不到等：</p><ul><li><p>空指针异常</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">at org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)</span><br><span class="line">at org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:33)</span><br><span class="line">at org.apache.zeppelin.spark.SparkInterpreter.createSparkContext_2(SparkInterpreter.java:348)</span><br><span class="line">at org.apache.zeppelin.spark.SparkInterpreter.createSparkContext(SparkInterpreter.java:337)</span><br><span class="line">at org.apache.zeppelin.spark.SparkInterpreter.getSparkContext(SparkInterpreter.java:142)</span><br></pre></td></tr></table></figure></li><li><p>连接拒绝</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: </span><br><span class="line">at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)</span><br><span class="line">at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)</span><br><span class="line">at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)</span><br><span class="line">at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:201)</span><br><span class="line">at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:65)</span><br><span class="line">at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:64)</span><br><span class="line">at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)</span><br><span class="line">... 4 more</span><br><span class="line">Caused by: java.io.IOException: Connection from alone/192.168.128.132:34711 closed</span><br></pre></td></tr></table></figure></li><li><p>方法找不到</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.NoSuchMethodError: org.apache.spark.network.util.AbstractFileRegion.transferred()J</span><br><span class="line">at org.apache.spark.network.util.AbstractFileRegion.transfered(AbstractFileRegion.java:28)</span><br><span class="line">at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:228)</span><br><span class="line">at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:282)</span><br><span class="line">at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:879)</span><br></pre></td></tr></table></figure></li></ul><h3 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h3><h4 id="相关说明"><a href="#相关说明" class="headerlink" title="相关说明"></a>相关说明</h4><p>首先，经过官方说明，如果想使用多个Spark，<font color="red">不能做的操作</font>就是直接修改Zeppelin的配置文件<code>zeppelin-env.sh</code>，向其中添加<code>SPARK_HOME</code>环境变量，而Ambari中则是对应向<code>Advanced zeppelin-env</code>中<code>zeppelin_env_content</code>添加环境变量，这样做的话就只能使用一个Spark了。因此如果已经有这个配置的话，需要将配置中<code>SPARK_HOME</code>这部分注释掉或者删除。</p><p>当然，如果只有一个Spark，可以在这里直接配置固定的即可。</p><h4 id="解决Spark1错误"><a href="#解决Spark1错误" class="headerlink" title="解决Spark1错误"></a>解决Spark1错误</h4><h5 id="查看错误日志"><a href="#查看错误日志" class="headerlink" title="查看错误日志"></a>查看错误日志</h5><p>Ambari安装的Zeppelin的日志存放位置在<code>/var/log/zeppelin</code>，打开路径就可以看到对应执行引擎的日志，Spark1引擎对应的日志文件名为<code>zeppelin-interpreter-spark-spark-zeppelin-${hostname}.log</code>，一系列错误如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: scala.reflect.api.JavaUniverse.runtimeMirror(Ljava/lang/ClassLoader;)Lscala/reflect/api/JavaMirrors$JavaMirror;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: error reading Scala signature of scala.package: scala.collection.immutable.Range.vali</span><br><span class="line">dateRangeBoundaries(Lscala/Function1;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.AbstractMethodError: org.apache.spark.network.protocol.MessageWithHeader.touch(Ljava/lang/Object;)Lio/netty/util/ReferenceCounted;</span><br><span class="line">at io.netty.util.ReferenceCountUtil.touch(ReferenceCountUtil.java:77)</span><br><span class="line">at io.netty.channel.DefaultChannelPipeline.touch(DefaultChannelPipeline.java:116)</span><br></pre></td></tr></table></figure><h5 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h5><p>因为没有在zeppelin-env中配置<code>SPARK_HOME</code>环境变量，Zeppelin找不到依赖，可是官方说明在执行引擎中配置就可以了啊？检查执行引擎确实也已经设置了？</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin问题处理/1567675888659.png" alt="1567675888659"></p><p>分析看来其中只找到了bin目录下的执行脚本，但是没有找到依赖，这是为什么呢？</p><p>zeppelin的执行引擎是分组的，spark和spark2的分组都是spark，而这个分组则对应着zeppelin安装目录下的interpreter目录中的一个文件夹。</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin问题处理/1567676458882.png" alt="1567676458882"></p><p><img src="//heltman.github.io/2019/09/05/Zeppelin问题处理/1567676550472.png" alt="1567676550472"></p><p>这个文件夹中会有对应执行引擎的jar包和相关依赖，查看<code>spark</code>文件夹中就会发现名为<code>zeppelin-spark_2.11-0.7.3.2.6.5.0-292.jar</code>的执行引擎和<code>dep</code>文件夹。</p><p>里面包含一个200多MB的依赖文件<code>zeppelin-spark-dependencies_2.11-0.7.3.2.6.5.0-292.jar</code>，解压打开文件发现其根目录<code>spark-version-info.properties</code>文件中指明了使用的Spark版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">version=2.3.0.2.6.5.0-292</span><br><span class="line">user=jenkins</span><br><span class="line">revision=b6b578c8dff89bfc07981ea3bda074262c3800ad</span><br><span class="line">branch=HEAD</span><br><span class="line">date=2018-05-11T07:55:12Z</span><br><span class="line">url=git@github.com:hortonworks/spark2.git</span><br></pre></td></tr></table></figure><p>那么就说明这个依赖包是针对Spark2的，因此属于这个组的Spark1如果没有在环境变量里指明<code>SPARK_HOME</code>，直接使用的话依赖确实可能有问题。</p><h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><p>直接将spark1的依赖<code>/usr/hdp/current/spark-client/lib/spark-assembly-1.6.3.2.6.5.0-292-hadoop2.7.3.2.6.5.0-292.jar</code>拷贝到这个目录<code>/usr/hdp/current/zeppelin/interpreter/spark/dep</code>，这样能够解决spark1报错的问题，但是因为修改了组模板，可能存在和Spark2的依赖冲突的情况，因此推荐的解决方法是修改Zeppelin中Spark1对应执行引擎的依赖，点击进入编辑：</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin问题处理/1567679649778.png" alt="1567679649778"></p><p>拉到最下方添加依赖：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/hdp/current/spark-client/lib/spark-assembly-1.6.3.2.6.5.0-292-hadoop2.7.3.2.6.5.0-292.jar</span><br></pre></td></tr></table></figure><p>或者如图中的<code>/usr/hdp/2.6.5.0-292/spark/lib/spark-hdp-assembly.jar</code>，因为<code>current/spark-client</code>是软链到<code>2.6.5.0-292/spark</code>，而<code>spark-hdp-assembly.jar</code>只是<code>spark-assembly-1.6.3.2.6.5.0-292-hadoop2.7.3.2.6.5.0-292.jar</code>的软连接而已，随便用哪个。</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin问题处理/1567679738798.png" alt="1567679738798"></p><p>点击保存后重启该执行引擎（上面编辑键右边一个），至此解决完毕。</p><h4 id="解决Spark2错误"><a href="#解决Spark2错误" class="headerlink" title="解决Spark2错误"></a>解决Spark2错误</h4><h5 id="查看错误日志-1"><a href="#查看错误日志-1" class="headerlink" title="查看错误日志"></a>查看错误日志</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Caused by: io.netty.channel.socket.ChannelOutputShutdownException: Channel output shutdown</span><br><span class="line">at io.netty.channel.AbstractChannel$AbstractUnsafe.shutdownOutput(AbstractChannel.java:587)</span><br><span class="line">... 22 more</span><br><span class="line">Caused by: java.lang.NoSuchMethodError: org.apache.spark.network.util.AbstractFileRegion.transferred()J</span><br><span class="line">at org.apache.spark.network.util.AbstractFileRegion.transfered(AbstractFileRegion.java:28)</span><br><span class="line">at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:228)</span><br><span class="line">at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:282)</span><br><span class="line">at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:879)</span><br><span class="line"></span><br><span class="line">Exit code: 13</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MultiException[java.lang.NoClassDefFoundError: com/fasterxml/jackson/jaxrs/json/JacksonJaxbJsonProvider, java.lang.NoClassDefFoundError: com/fasterxml/jackson/jaxrs/json/JacksonJaxbJsonProvider]</span><br><span class="line"></span><br><span class="line">Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: </span><br><span class="line">at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)</span><br><span class="line">at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)</span><br><span class="line">at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)</span><br><span class="line">at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:201)</span><br><span class="line">at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:65)</span><br><span class="line">at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:64)</span><br><span class="line">at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)</span><br><span class="line">... 4 more</span><br><span class="line">Caused by: java.io.IOException: Connection from alone/192.168.128.132:34711 closed</span><br><span class="line"></span><br><span class="line">Exit code: 1</span><br></pre></td></tr></table></figure><h5 id="问题分析-1"><a href="#问题分析-1" class="headerlink" title="问题分析"></a>问题分析</h5><p>根据Spark1中的分析，Spark2显然不是相同的问题，当然错误日志也可以看得很明显，前一个找不到netty的某一个方法，后一个是找不到jackson的类，因此是依赖有问题。</p><p>检查Zeppelin的依赖目录<code>/usr/hdp/current/zeppelin/lib</code>，其中netty版本为<code>4.0.52.Final</code>，jackson版本为<code>2.5.4</code></p><p>检查Spark2的依赖目录<code>/usr/hdp/current/spark2-client/jars</code>，其中netty版本为<code>3.9.9.Final</code>和<code>4.1.17.Final</code>，jackson版本为<code>2.6.7</code></p><p>好了，知道怎么做了！</p><h5 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h5><p>知道原因，解决就很简单了，只需要将Zeppelin中对应依赖删除换成Spark的即可。</p><p>删除Zeppelin的lib中<code>netty-all-4.0.52.Final.jar</code>，将spark2中jars下<code>netty-3.9.9.Final.jar</code>，<code>netty-all-4.1.17.Final.jar</code>移到目录下</p><p>删除Zeppelin的lib中<code>jackson-annotations-2.5.4.jar</code>、<code>jackson-core-2.5.4.jar</code>、<code>jackson-databind-2.5.4.jar</code>，将spark2中jars下<code>jackson-annotations-2.6.7.jar</code>、<code>jackson-core-2.6.7.jar</code>、<code>jackson-databind-2.6.7.jar</code></p><h3 id="解决结果"><a href="#解决结果" class="headerlink" title="解决结果"></a>解决结果</h3><p>解决后两个引擎都能正常运行，完美！</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin问题处理/1567681494806.png" alt="1567681494806"></p><h2 id="Zeppelin取消账户密码"><a href="#Zeppelin取消账户密码" class="headerlink" title="Zeppelin取消账户密码"></a>Zeppelin取消账户密码</h2><h3 id="问题说明-1"><a href="#问题说明-1" class="headerlink" title="问题说明"></a>问题说明</h3><p>使用Ambari安装的Zeppelin自动配置了账号密码登录，这样比较安全，默认账户密码是admin:admin，但是为了方便起见，想要不登录直接编辑可以吗？那是自然！只要开启匿名登录即可。</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin问题处理/1567682528350.png" alt="1567682528350"></p><h3 id="解决过程-1"><a href="#解决过程-1" class="headerlink" title="解决过程"></a>解决过程</h3><p>过程较为简单，主要分为以下三步。</p><h4 id="启用匿名登录"><a href="#启用匿名登录" class="headerlink" title="启用匿名登录"></a>启用匿名登录</h4><p>Ambari界面可以直接在<code>Advanced zeppelin-config</code>中修改<code>zeppelin.anonymous.allowed</code>为<code>true</code>即可（原本为<code>false</code>）.</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin问题处理/1567682019709.png" alt="1567682019709"></p><h4 id="关闭验证登录"><a href="#关闭验证登录" class="headerlink" title="关闭验证登录"></a>关闭验证登录</h4><p>Ambari界面直接在<code>Advanced zeppelin-shiro.ini</code>中修改<code>shiro_ini_content</code>的最后，打开<code>/** = anon</code>的注释，注释掉<code>/** = authc</code>，改完如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[urls]</span><br><span class="line"># This section is used for url-based security.</span><br><span class="line"># You can secure interpreter, configuration and credential information by urls. Comment or uncomment the below urls that you want to hide.</span><br><span class="line"># anon means the access is anonymous.</span><br><span class="line"># authc means Form based Auth Security</span><br><span class="line"># To enfore security, comment the line below and uncomment the next one</span><br><span class="line">/api/version = anon</span><br><span class="line">#/api/interpreter/** = authc, roles[admin]</span><br><span class="line">#/api/configurations/** = authc, roles[admin]</span><br><span class="line">#/api/credential/** = authc, roles[admin]</span><br><span class="line">/** = anon</span><br><span class="line">#/** = authc</span><br></pre></td></tr></table></figure><h4 id="重启服务生效"><a href="#重启服务生效" class="headerlink" title="重启服务生效"></a>重启服务生效</h4><p>保存后按照要求重启服务即可</p><h3 id="解决结果-1"><a href="#解决结果-1" class="headerlink" title="解决结果"></a>解决结果</h3><p><img src="//heltman.github.io/2019/09/05/Zeppelin问题处理/1567683023745.png" alt="1567683023745"></p>]]></content>
      
      
      <categories>
          
          <category> 问题处理 </category>
          
          <category> Zeppelin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Zeppelin </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive问题处理</title>
      <link href="/2019/09/04/Hive%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/"/>
      <url>/2019/09/04/Hive%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<center>Hive使用中常见问题的整合处理，方便以后查询。</center><a id="more"></a><h1 id="注释问题"><a href="#注释问题" class="headerlink" title="注释问题"></a>注释问题</h1><h2 id="注释中文乱码"><a href="#注释中文乱码" class="headerlink" title="注释中文乱码"></a>注释中文乱码</h2><h4 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h4><p>创建表的时候，如果添加了comment字段且包含中文，创建后查询时为乱码。</p><h4 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h4><p>登录保存Hive元数据的数据库，修改<code>表</code>、<code>分区</code>、<code>视图</code>的编码为<code>utf8</code>即可，以下以MySQL为例——</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> hive;</span><br><span class="line"><span class="comment">--修改表字段注解和表注解</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> COLUMNS_V2 <span class="keyword">modify</span> <span class="keyword">column</span> <span class="keyword">COMMENT</span> <span class="built_in">varchar</span>(<span class="number">256</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> TABLE_PARAMS <span class="keyword">modify</span> <span class="keyword">column</span> PARAM_VALUE <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="comment">--修改分区字段注解</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_PARAMS <span class="keyword">modify</span> <span class="keyword">column</span> PARAM_VALUE <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_KEYS <span class="keyword">modify</span> <span class="keyword">column</span> PKEY_COMMENT <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="comment">--修改索引注解</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> INDEX_PARAMS <span class="keyword">modify</span> <span class="keyword">column</span> PARAM_VALUE <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure><blockquote><p>注意：已经创建的表还会是乱码，请删除后重建，注意内部表删除会删除数据！</p></blockquote><h4 id="解决结果"><a href="#解决结果" class="headerlink" title="解决结果"></a>解决结果</h4><p>已解决</p><h2 id="使用SerDe时注释变为from-deserializer"><a href="#使用SerDe时注释变为from-deserializer" class="headerlink" title="使用SerDe时注释变为from deserializer"></a>使用SerDe时注释变为from deserializer</h2><h4 id="问题说明-1"><a href="#问题说明-1" class="headerlink" title="问题说明"></a>问题说明</h4><p>当使用非内置SerDe时，添加了注释的话，字段注释会显示成<code>from deserializer</code></p><h4 id="解决过程-1"><a href="#解决过程-1" class="headerlink" title="解决过程"></a>解决过程</h4><p>这个问题官方目前尚未解决，但是可以设置Hive如下属性来能够正确识别注释——</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.serdes.using.metastore.for.schema=org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe;</span><br></pre></td></tr></table></figure><blockquote><p>参考：</p><p>Hive column comments disappearing/being replaced by “from deserializer” <span class="exturl" data-url="aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9ISVZFLTE1Mzc0" title="https://issues.apache.org/jira/browse/HIVE-15374">HIVE-15374<i class="fa fa-external-link"></i></span></p><p>Hive table always set column comment is “from deserializer” <span class="exturl" data-url="aHR0cHM6Ly9zdGFja292ZXJmbG93LmNvbS9xdWVzdGlvbnMvNDgwNTYxODcvaGl2ZS10YWJsZS1hbHdheXMtc2V0LWNvbHVtbi1jb21tZW50LWlzLWZyb20tZGVzZXJpYWxpemVy" title="https://stackoverflow.com/questions/48056187/hive-table-always-set-column-comment-is-from-deserializer">STACKOVERFLOW<i class="fa fa-external-link"></i></span></p></blockquote><h4 id="解决结果-1"><a href="#解决结果-1" class="headerlink" title="解决结果"></a>解决结果</h4><p>已解决</p>]]></content>
      
      
      <categories>
          
          <category> 问题处理 </category>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ambari插件问题处理</title>
      <link href="/2019/09/02/Ambari%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/"/>
      <url>/2019/09/02/Ambari%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<center>使用Ambari安装完HDP后，其中的插件使用或多或少存在一些问题，以下收集了一些解决方案供参考。</center><a id="more"></a><h3 id="Files-View插件"><a href="#Files-View插件" class="headerlink" title="Files View插件"></a>Files View插件</h3><p><strong>问题：</strong>进行操作时弹出权限拒绝</p><p><strong>解决方法：</strong></p><p>在Ambari界面修改HDFS的配置中Custom core-site，新增：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop.proxyuser.admin.groups=*</span><br><span class="line">hadoop.proxyuser.admin.hosts=*</span><br></pre></td></tr></table></figure><p>其中admin为报没有权限的用户名</p><p>如果配置后仍然报没有权限，说明部署Ambari-server的机器上没有该用户，需要登录机器新增该用户，并添加到users、hdfs和hadoop组：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">useradd -G hdfs admin </span><br><span class="line">usermod -a -G users admin </span><br><span class="line">usermod -a -G hadoop admin</span><br></pre></td></tr></table></figure><p>现在应该有权限进行操作了</p><h3 id="Hive-View和Hive-View-2-0插件"><a href="#Hive-View和Hive-View-2-0插件" class="headerlink" title="Hive View和Hive View 2.0插件"></a>Hive View和Hive View 2.0插件</h3><p><strong>问题：</strong>数据存储异常，主要报错如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.ambari.view.PersistenceException: Caught exception trying to store view entity JobImpl&#123;id='null, owner='admin, hiveQueryId='null, dagId='null, queryId='null&#125;</span><br><span class="line">......</span><br><span class="line">Caused by: javax.persistence.RollbackException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException</span><br><span class="line">Internal Exception: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'ambari.ds_jobimpl_5' doesn't exist</span><br><span class="line">Error Code: 1146</span><br><span class="line">Call: INSERT INTO DS_JOBIMPL_5 (DS_id, DS_applicationId, DS_confFile, DS_dagId, DS_dagName, DS_dataBase, DS_dateSubmitted, DS_duration, DS_forcedContent, DS_globalSettings, DS_guid, DS_hiveQueryId, DS_logFile, DS_owner, DS_queryFile, DS_queryId, DS_referrer, DS_sessionTag, DS_sqlState, DS_status, DS_statusDir, DS_statusMessage, DS_title) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)</span><br></pre></td></tr></table></figure><p><strong>解决方法：</strong>需要到Mysql的ambari数据库中建立相应表，建表语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> DS_JOBIMPL_5 (</span><br><span class="line">ds_id <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">ds_applicationid <span class="built_in">TEXT</span>,</span><br><span class="line">ds_conffile <span class="built_in">TEXT</span>,</span><br><span class="line">ds_dagid <span class="built_in">TEXT</span>,</span><br><span class="line">ds_dagname <span class="built_in">TEXT</span>,</span><br><span class="line">ds_database <span class="built_in">TEXT</span>,</span><br><span class="line">ds_datesubmitted <span class="built_in">BIGINT</span>,</span><br><span class="line">ds_duration <span class="built_in">BIGINT</span>,</span><br><span class="line">ds_forcedcontent <span class="built_in">TEXT</span>,</span><br><span class="line">ds_globalsettings <span class="built_in">TEXT</span>,</span><br><span class="line">DS_guid <span class="built_in">TEXT</span>,</span><br><span class="line">DS_hiveQueryId <span class="built_in">TEXT</span>,</span><br><span class="line">ds_logfile <span class="built_in">TEXT</span>,</span><br><span class="line">ds_owner <span class="built_in">TEXT</span>,</span><br><span class="line">ds_queryfile <span class="built_in">TEXT</span>,</span><br><span class="line">ds_queryid <span class="built_in">TEXT</span>,</span><br><span class="line">ds_referrer <span class="built_in">TEXT</span>,</span><br><span class="line">ds_sessiontag <span class="built_in">TEXT</span>,</span><br><span class="line">ds_sqlstate <span class="built_in">TEXT</span>,</span><br><span class="line">ds_status <span class="built_in">TEXT</span>,</span><br><span class="line">ds_statusdir <span class="built_in">TEXT</span>,</span><br><span class="line">ds_statusmessage <span class="built_in">TEXT</span>,</span><br><span class="line">ds_title <span class="built_in">TEXT</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ds_id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 问题处理 </category>
          
          <category> Ambari </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ambari </tag>
            
            <tag> 插件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库架构简单介绍</title>
      <link href="/2019/08/28/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/"/>
      <url>/2019/08/28/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<center>针对数据仓库架构的相关概念进行简单的解释说明，对数据仓库的建模方式进行简单的描述。</center><a id="more"></a><p><img src="//heltman.github.io/2019/08/28/数据仓库架构/%E5%A4%A7%E6%95%B0%E6%8D%AE.jpg" alt="大数据"></p><h2 id="数据仓库概念"><a href="#数据仓库概念" class="headerlink" title="数据仓库概念"></a>数据仓库概念</h2><h3 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h3><h4 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h4><ul><li>需求的变化</li><li>业务系统的建设逐渐完善</li><li>分析类需求不断增加</li><li>不断增加的信息孤岛导致数据集成问题不断增加</li></ul><h4 id="技术发展状况"><a href="#技术发展状况" class="headerlink" title="技术发展状况"></a>技术发展状况</h4><ul><li>关系数据库技术日趋成熟</li><li>报表和复杂查询处理起来非常困难</li><li>各个系统之间数据不一致</li></ul><p>常用关系型数据仓库：greenplum，vertica</p><h4 id="ODS：操作型数据仓库"><a href="#ODS：操作型数据仓库" class="headerlink" title="ODS：操作型数据仓库"></a>ODS：操作型数据仓库</h4><p>早期数据仓库模型，为企业提供即时的，操作型的，继承的数据集合，具有面向主题性，集成性，动态性，即时性，明细性等特性。面向即时的，一般是一小时，到一分钟。存储时间较短，7天5天等。基于战术的操作，和基于战略的数据仓库有所不同。</p><h4 id="STAGING-AREA（缓存区，分段存储区）"><a href="#STAGING-AREA（缓存区，分段存储区）" class="headerlink" title="STAGING AREA（缓存区，分段存储区）"></a>STAGING AREA（缓存区，分段存储区）</h4><p>为了保证数据移动的顺利进行而开设的阶段性数据存储空间，和ODS有重合，有ODS可以不要这个。如果数据仓库数据有问题可以直接从缓存区拿取数据，不需要从业务处拿取。</p><h4 id="DATA-MART（数据集市）"><a href="#DATA-MART（数据集市）" class="headerlink" title="DATA MART（数据集市）"></a>DATA MART（数据集市）</h4><p>数据集市是完整的数据仓库的一个逻辑子集，为了特定的应用目的或应用范围，而从数据仓库独立出来的一部分数据。，也可称为部门数据或主题数据。</p><h4 id="数据仓库设计"><a href="#数据仓库设计" class="headerlink" title="数据仓库设计"></a>数据仓库设计</h4><blockquote><p><strong>源系统 –&gt; ODS –&gt; DW –&gt; DM</strong></p></blockquote><p>建立数据仓库与数据集市，一般采用“自顶向下”和“自下而上”相结合的设计思想。</p><h4 id="OLAP（在线分析处理）"><a href="#OLAP（在线分析处理）" class="headerlink" title="OLAP（在线分析处理）"></a>OLAP（在线分析处理）</h4><p>OLAP是一种多为分析技术，用来满足决策用户在大量的业务数据中，从多角度探索业务活动的规律性、市场的运作趋势的分析需求，并辅助他们进行战略决策的制定。OLAP系统按照存储格式可以分为关系OLAP（ROLAP）、多维OLAP（MOLAP）和混合型OLAP（HOLAP）三种类型。Kylin属于多维OLAP</p><h4 id="OLAP-VA-OLTP（在线分析系统和在线交易系统）"><a href="#OLAP-VA-OLTP（在线分析系统和在线交易系统）" class="headerlink" title="OLAP VA OLTP（在线分析系统和在线交易系统）"></a>OLAP VA OLTP（在线分析系统和在线交易系统）</h4><table><thead><tr><th>ITEM</th><th>OLTP</th><th>OLAP</th></tr></thead><tbody><tr><td>用户</td><td>操作人员</td><td>分析决策人员，高级管理人员</td></tr><tr><td>功能</td><td>日常操作处理</td><td>分析决策</td></tr><tr><td>设计</td><td>面向应用</td><td>面向主题</td></tr><tr><td>数据</td><td>当前的，最新的细节的，二维的，分立的</td><td>历史的，聚集的，多维的，集成的，统一的</td></tr><tr><td>存取</td><td>读/写数十条记录</td><td>读上百万条记录</td></tr><tr><td>工作单位</td><td>简单的读写</td><td>复杂查询</td></tr><tr><td>DB大小</td><td>GB到TB</td><td>TB-PB级别</td></tr><tr><td>度量</td><td>事物吞吐量</td><td>查询吞吐量、响应时间</td></tr></tbody></table><hr><h2 id="数据仓库建模"><a href="#数据仓库建模" class="headerlink" title="数据仓库建模"></a>数据仓库建模</h2><p>范式建模、维度建模、Data Valut、Anchor</p><h3 id="范式建模：ER模型"><a href="#范式建模：ER模型" class="headerlink" title="范式建模：ER模型"></a>范式建模：ER模型</h3><p>自上而下（EDW-DM）的数据仓库架构。操作型或事务型系统的数据源，通过ETL抽取转换和加载到数据仓库的ODS层，然后通过ODS的数据建设原子数据的数据仓库EDW，EDW不是多维格式的，不方便上层应用做数据分析，所以需要通过汇总建设成多维格式的数据集市层。</p><p>优势：易于维护，高度集成</p><p>劣势：结构死板，部署周期较长</p><h3 id="维度建模"><a href="#维度建模" class="headerlink" title="维度建模"></a>维度建模</h3><p>以分析决策的需求出发构建模型，构建的数据模型为分析需求服务，因此它重点解决用户如何更快速完成分析需求，同时还有叫好的大规模复杂查询的响应性能。</p><p>维度模型是由一个规范化的事实表和反规范化的一些维度组成的</p><ul><li>一种非规范化的关系模型</li><li>表和表之间的关系通过关键字和外键来定义</li></ul><p>以良好的可理解性和方便的产生报表来进行数据组织，很少考虑修改的性能</p><p>通过SQL或者相关的工具实现数据的查询和维护</p><h4 id="维度表"><a href="#维度表" class="headerlink" title="维度表"></a>维度表</h4><p>每一张维度表对应现实世界中的一个对象或者概念</p><ul><li>例如：客户、产品、日期、地区、商场</li></ul><p>维度表的特征：</p><ul><li>包含了众多描述性的列：维度表的范围很宽（具有多个属性）</li><li>通常情况下，跟事实表相比，行数相对较小：通常&lt;10万条</li><li>内容相对固定：几乎就是一个类查找表或者编码表</li></ul><h4 id="事实表"><a href="#事实表" class="headerlink" title="事实表"></a>事实表</h4><p>每一个事实表通常包含了处理所关心的度量值</p><p>每一个事实表的行包括</p><ul><li>具有可加性的数值型的度量值</li><li>与维度表相连接的外键<ul><li>通常具有两个和两个以上的外键</li><li>外键之间表示维度表之间多对多的关系</li></ul></li></ul><p>事实表的特征</p><ul><li>非常大：包含几万、几十万甚至千万的记录</li><li>内容相对的窄，列数较少</li><li>经常发生变化</li></ul><h4 id="维度模型设计过程"><a href="#维度模型设计过程" class="headerlink" title="维度模型设计过程"></a>维度模型设计过程</h4><ol><li>选择业务过程</li><li>声明粒度</li><li>确定维度</li><li>确定事实</li></ol><hr><h2 id="数据治理"><a href="#数据治理" class="headerlink" title="数据治理"></a>数据治理</h2><h3 id="元数据管理"><a href="#元数据管理" class="headerlink" title="元数据管理"></a>元数据管理</h3><ul><li>技术元数据<ul><li>表级血缘分析</li><li>字段级别血缘分析</li><li>影响分析</li><li>数据审计</li></ul></li><li>业务元数据</li></ul><h3 id="主数据管理"><a href="#主数据管理" class="headerlink" title="主数据管理"></a>主数据管理</h3><hr><h2 id="数据仓库架构"><a href="#数据仓库架构" class="headerlink" title="数据仓库架构"></a>数据仓库架构</h2><p><img src="//heltman.github.io/2019/08/28/数据仓库架构/1558625207687.png" alt="1558625207687"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 入门文档 </tag>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark简单学习说明</title>
      <link href="/2019/08/28/Spark%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/08/28/Spark%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<center>Spark简单解释说明，待更新……</center><a id="more"></a><h2 id="配置Spark"><a href="#配置Spark" class="headerlink" title="配置Spark"></a>配置Spark</h2><p>连接Spark到任意hadoop集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### in conf/spark-env.sh ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If 'hadoop' binary is on your PATH</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(hadoop classpath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># With explicit path to 'hadoop' binary</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/path/to/hadoop/bin/hadoop classpath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Passing a Hadoop configuration directory</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(hadoop --config /path/to/configs classpath)</span><br></pre></td></tr></table></figure><h2 id="写Spark程序"><a href="#写Spark程序" class="headerlink" title="写Spark程序"></a>写Spark程序</h2><h3 id="与Spark连接"><a href="#与Spark连接" class="headerlink" title="与Spark连接"></a>与Spark连接</h3><p>Spark 1.6.1使用Scala 2.10。要在Scala中编写应用程序，您需要使用兼容的Scala版本（例如2.10.X）。</p><h4 id="编写Spark程序所需mvn依赖"><a href="#编写Spark程序所需mvn依赖" class="headerlink" title="编写Spark程序所需mvn依赖"></a>编写Spark程序所需mvn依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果需要连接HDFS，还需要如下依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>最后需要在程序中import一些Spark类</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br></pre></td></tr></table></figure><p>（在Spark 1.3.0之前，您需要显式导入org.apache.spark.SparkContext._以启用必要的隐式转换。)</p><h3 id="初始化Spark"><a href="#初始化Spark" class="headerlink" title="初始化Spark"></a>初始化Spark</h3><p>Spark程序必须做的第一件事是创建一个SparkContext对象，它告诉Spark如何访问集群。要创建SparkContext，首先需要构建一个包含有关应用程序信息的SparkConf对象。</p><p>每个JVM只能激活一个SparkContext。在创建新的SparkContext之前，必须先<code>stop()</code>。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(appName).setMaster(master)</span><br><span class="line"><span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure><p>appName参数是应用程序在集群UI上显示的名称。 master是Spark，Mesos或YARN群集URL，或者是以本地模式运行的特殊“本地”字符串。 实际上，在群集上运行时，您不希望在程序中对master进行硬编码，而是使用spark-submit启动应用程序并在那里接收它。 但是，对于本地测试和单元测试，您可以传递“local”以在进程中运行Spark。</p><h4 id="使用shell"><a href="#使用shell" class="headerlink" title="使用shell"></a>使用shell</h4><p>在Spark shell中，已经在名为<code>sc</code>的变量中为您创建了一个特殊的SparkContext 。制作自己的SparkContext将无法正常工作。您可以使用<code>--master</code>参数设置上下文连接到的主服务器，并且可以通过将逗号分隔的列表传递给参数来将JAR添加到类路径中<code>--jars</code>。您还可以通过为参数提供以逗号分隔的maven坐标列表，将依赖项（例如Spark包）添加到shell会话中<code>--packages</code>。任何可能存在依赖关系的其他存储库（例如SonaType）都可以传递给<code>--repositories</code>参数。例如，要<code>bin/spark-shell</code>在四个核心上运行，请使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-shell --master <span class="built_in">local</span>[4]</span><br></pre></td></tr></table></figure><p>或者，要将code.jar添加到其类路径中，请使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-shell --master <span class="built_in">local</span>[4] --jars code.jar</span><br></pre></td></tr></table></figure><p>要使用maven坐标包含依赖项:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-shell --master <span class="built_in">local</span>[4] --packages <span class="string">"org.example:example:0.1"</span></span><br></pre></td></tr></table></figure><p>有关选项的完整列表，请运行<code>spark-shell --help</code>。在幕后， <code>spark-shell</code>调用更通用的<span class="exturl" data-url="aHR0cDovL3NwYXJrLmFwYWNoZS5vcmcvZG9jcy8xLjYuMS9zdWJtaXR0aW5nLWFwcGxpY2F0aW9ucy5odG1s" title="http://spark.apache.org/docs/1.6.1/submitting-applications.html"><code>spark-submit</code>脚本<i class="fa fa-external-link"></i></span>。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 入门文档 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
