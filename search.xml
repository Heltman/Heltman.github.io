<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hive连接ES</title>
      <link href="/2019/09/11/Hive%E8%BF%9E%E6%8E%A5ES/"/>
      <url>/2019/09/11/Hive%E8%BF%9E%E6%8E%A5ES/</url>
      
        <content type="html"><![CDATA[<center>Hive连接ES创建外部表是一个比较方便的方式来在Hive和ES之间传递数据，下面来介绍具体实现。<a id="more"></a><p>先介绍环境，ES版本2.4.4，Hive版本采用HDP2.6.5自带的1.2.1。因此选用elasticsearch-hadoop的版本为2.4.4。</p><blockquote><p>elasticsearch-hadoop </p><p>官网下载地址：<span class="exturl" data-url="aHR0cHM6Ly93d3cuZWxhc3RpYy5jby9jbi9kb3dubG9hZHMvcGFzdC1yZWxlYXNlcyNlcy1oYWRvb3A=" title="https://www.elastic.co/cn/downloads/past-releases#es-hadoop">https://www.elastic.co/cn/downloads/past-releases#es-hadoop<i class="fa fa-external-link"></i></span></p><p>官方文档地址：<span class="exturl" data-url="aHR0cHM6Ly93d3cuZWxhc3RpYy5jby9ndWlkZS9lbi9lbGFzdGljc2VhcmNoL2hhZG9vcC9tYXN0ZXIvcmVmZXJlbmNlLmh0bWw=" title="https://www.elastic.co/guide/en/elasticsearch/hadoop/master/reference.html">https://www.elastic.co/guide/en/elasticsearch/hadoop/master/reference.html<i class="fa fa-external-link"></i></span></p><p>github地址：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2VsYXN0aWMvZWxhc3RpY3NlYXJjaC1oYWRvb3A=" title="https://github.com/elastic/elasticsearch-hadoop">https://github.com/elastic/elasticsearch-hadoop<i class="fa fa-external-link"></i></span></p></blockquote><p>下载elasticsearch-hadoop-2.4.4.zip文件，解压后得到其中的elasticsearch-hadoop-hive-2.4.4.jar，将依赖添加到Hive，参见《Hive添加依赖》一章。下文假设已经添加好依赖了，使用beeline来操作。</p><h3 id="创建ElaticSearch映射外部表"><a href="#创建ElaticSearch映射外部表" class="headerlink" title="创建ElaticSearch映射外部表"></a>创建ElaticSearch映射外部表</h3><p>使用如下语句创建</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> hive_es_external_user (</span><br><span class="line">  <span class="keyword">id</span> <span class="keyword">string</span>,</span><br><span class="line">  username <span class="keyword">string</span>,</span><br><span class="line">  <span class="keyword">password</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="built_in">number</span> <span class="built_in">bigint</span>)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'org.elasticsearch.hadoop.hive.EsStorageHandler'</span></span><br><span class="line">TBLPROPERTIES(</span><br><span class="line">  <span class="string">'es.nodes'</span> = <span class="string">'es1:9200'</span>,</span><br><span class="line">  <span class="string">'es.index.auto.create'</span> = <span class="string">'false'</span>,</span><br><span class="line">  <span class="string">'es.resource'</span> = <span class="string">'user_list/user'</span>,</span><br><span class="line">  <span class="string">'es.read.metadata'</span> = <span class="string">'true'</span>,</span><br><span class="line">  <span class="string">'es.mapping.id'</span> = <span class="string">'id'</span>,</span><br><span class="line">  <span class="string">'es.mapping.names'</span> = <span class="string">'id:_metadata._id,username:username, password:password, number:number'</span>);</span><br></pre></td></tr></table></figure><h3 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h3><h4 id="问题1-时间转换异常"><a href="#问题1-时间转换异常" class="headerlink" title="问题1 时间转换异常"></a>问题1 时间转换异常</h4><p>一般存ES的时间数据会保存成<code>yyyy-MM-dd HH:mm:ss</code>格式，如果要映射时间字段，创建表会成功，但是查询时则会抛出时间转换异常：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [2019-01-01 11:30:03] for field [start_time] at </span><br><span class="line">...</span><br><span class="line">Caused by: java.lang.IllegalArgumentException: Invalid format: "2018-10-10 10:20:10" is malformed at " 10:20:10" at org.joda.time.format.DateTimeFormatter.parseDateTime(DateTimeFormatter.java:945)</span><br></pre></td></tr></table></figure><p><strong>解决方法1</strong></p><p>这是因为es-hadoop连接插件使用joda-Time中ISO8601来格式化日期，支持日期格式为<code>yyyy-MM-dd&#39;T&#39;HH:mm:ss.SSSZZ</code>，因此不能解析<code>yyyy-MM-dd HH:mm:ss</code>格式，因此可以设置添加如下配置关闭ES的解析时间类型功能：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">'es.mapping.date.rich'='false'</span><br></pre></td></tr></table></figure><p>但是这里还有另一个坑，比如关闭解析功能后，创建外部表如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> hive_es_external_user_auth(</span><br><span class="line">  areaname <span class="keyword">string</span>,</span><br><span class="line">  beginTime <span class="built_in">timestamp</span>)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'org.elasticsearch.hadoop.hive.EsStorageHandler'</span></span><br><span class="line">TBLPROPERTIES(</span><br><span class="line">  <span class="string">'es.nodes'</span> = <span class="string">'es1:9200'</span>,</span><br><span class="line">  <span class="string">'es.index.auto.create'</span> = <span class="string">'false'</span>,</span><br><span class="line">  <span class="string">'es.resource'</span> = <span class="string">'user_area/user'</span>,</span><br><span class="line">  <span class="string">'es.read.metadata'</span> = <span class="string">'true'</span>,</span><br><span class="line">  <span class="string">'es.mapping.id'</span> = <span class="string">'id'</span>,</span><br><span class="line">  <span class="string">'es.mapping.date.rich'</span>=<span class="string">'false'</span>,</span><br><span class="line">  <span class="string">'es.mapping.names'</span> = <span class="string">'areaname:areaName, beginTime:beginTime'</span>);</span><br></pre></td></tr></table></figure><p>查看数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> hive_es_external_user_auth <span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><p>这时会报出另外一个错误：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to org.apache.hadoop.hive.serde2.io.TimestampWritable</span><br></pre></td></tr></table></figure><p>因为字段类型被设置成了timestamp，但是你不解析的话直接查询返回的是text流数据，timestamp类解析不了…</p><p>因此类型老老实实设置成string吧……不过另一方面，直接建立映射表查询ES，这绝对不是一个好的方案，这正是因为下面会解释的<strong>问题2</strong>。</p><p><strong>解决方法2</strong></p><p>自定义时间解析类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.io.TimestampWritable;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.hadoop.cfg.Settings;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.hadoop.hive.HiveValueReader;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.xml.bind.DatatypeConverter;</span><br><span class="line"><span class="keyword">import</span> java.sql.Timestamp;</span><br><span class="line"><span class="keyword">import</span> java.text.ParseException;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EsValueReader</span> <span class="keyword">extends</span> <span class="title">HiveValueReader</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String dateFormat;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> SimpleDateFormat format_ymd_hms;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSettings</span><span class="params">(Settings settings)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.setSettings(settings);</span><br><span class="line">        <span class="comment">//获取我们自定的时间格式</span></span><br><span class="line">        dateFormat = settings.getProperty(<span class="string">"es.date.format"</span>);</span><br><span class="line">        format_ymd_hms = <span class="keyword">new</span> SimpleDateFormat(dateFormat);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> Object <span class="title">parseDate</span><span class="params">(String value, <span class="keyword">boolean</span> richDate)</span> </span>&#123;</span><br><span class="line">        Date d = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">if</span> (!<span class="string">""</span>.equals(dateFormat) &amp;&amp; dateFormat != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                d = format_ymd_hms.parse(value);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (ParseException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            d = DatatypeConverter.parseDateTime(value).getTime();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> (richDate ? <span class="keyword">new</span> TimestampWritable(<span class="keyword">new</span> Timestamp(d.getTime())) : parseString(value));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打包后添加到Hive依赖，然后用如下建表语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> hive_es_external_user_auth(</span><br><span class="line">  areaname <span class="keyword">string</span>,</span><br><span class="line">  beginTime <span class="built_in">timestamp</span>)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'org.elasticsearch.hadoop.hive.EsStorageHandler'</span></span><br><span class="line">TBLPROPERTIES(</span><br><span class="line">  <span class="string">'es.nodes'</span> = <span class="string">'es1:9200'</span>,</span><br><span class="line">  <span class="string">'es.index.auto.create'</span> = <span class="string">'false'</span>,</span><br><span class="line">  <span class="string">'es.resource'</span> = <span class="string">'user_area/user'</span>,</span><br><span class="line">  <span class="string">'es.read.metadata'</span> = <span class="string">'true'</span>,</span><br><span class="line">  <span class="string">'es.mapping.id'</span> = <span class="string">'id'</span>,</span><br><span class="line">  <span class="string">'es.date.format'</span> = <span class="string">'yyyy-MM-dd HH:mm:ss'</span>, </span><br><span class="line"><span class="string">'es.ser.reader.value.class'</span> = <span class="string">'com.eshadoop.EsValueReader'</span>,</span><br><span class="line">  <span class="string">'es.mapping.names'</span> = <span class="string">'areaname:areaName, beginTime:beginTime'</span>);</span><br></pre></td></tr></table></figure><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x0bGl5dWUvYXJ0aWNsZS9kZXRhaWxzLzgwMDIxMDU4" title="https://blog.csdn.net/ltliyue/article/details/80021058">参考链接<i class="fa fa-external-link"></i></span></p><p>解决方法3**</p><p>不过这个时间问题还有另外的解决方案，从ES本身下文章，当然直接设置保存格式为ISO8601就没事了，但是如果想按照<code>yyyy-MM-dd HH:mm:ss</code>存，按照ISO8601查询，那么可以按照如下设置：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">"beginTime": &#123;</span><br><span class="line">"format": "yyyy-MM-dd HH:mm:ss || yyyy-MM-dd'T'HH:mm:ss.SSSZZ",</span><br><span class="line">"type": "date"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="问题2-几乎不存在的下推查询"><a href="#问题2-几乎不存在的下推查询" class="headerlink" title="问题2 几乎不存在的下推查询"></a>问题2 几乎不存在的下推查询</h4><p>暂未发现Hive查询ES表会进行任何下推操作，所以映射ES表后进行的任何查询都会把ES所有数据取回然后过滤，小数据量表还没什么，大数据表根本不能用。即使是最简单的limit子句，也会将所有数据导回然后过滤，聚合函数count什么的就不说了。</p><p>解决办法是没有的，但是如果只是想简单查看表的一部分数据，倒是可以添加如下参数设置搜索限制：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">'es.scoll.limit'='10'</span><br></pre></td></tr></table></figure><p>但是这个操作并不是只返回10条的意思，而是每个分片只返回前10条，所以如果有3个分片，则返回30条。意思大概是你映射了表每个分片的前10条。</p></center>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive排除选择列</title>
      <link href="/2019/09/09/Hive%E6%8E%92%E9%99%A4%E9%80%89%E6%8B%A9%E5%88%97/"/>
      <url>/2019/09/09/Hive%E6%8E%92%E9%99%A4%E9%80%89%E6%8B%A9%E5%88%97/</url>
      
        <content type="html"><![CDATA[<center>有时候表字段很多，但是在`SELECT`语句中有几个字段不需要，要是能排除某些字段就好了，Hive是支持这个操作的，当然也有注意事项</center><a id="more"></a><h3 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h3><p>在Hive中表字段较多时，需要选择其中大部分字段，往往是很痛苦的事情，比如创建了两个相同的分区表A和B，然后想将A的某个分区插入B的某个分区，使用如下语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> B <span class="keyword">PARTITION</span>(<span class="keyword">MONTH</span>=<span class="number">201901</span>,<span class="keyword">DAY</span>=<span class="number">01</span>) <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> A;</span><br></pre></td></tr></table></figure><p>会出现错误字段数不匹配，因为Hive的插入数据语法中不会检测分区字段，因此表B比表A少了2个分区字段，字段数不匹配，这就会报错，因此需要在后面不使用<code>*</code>，而是用每一个字段名，可是表A字段多，写的太累了。</p><h3 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h3><p>根据<span class="exturl" data-url="aHR0cHM6Ly9jd2lraS5hcGFjaGUub3JnL2NvbmZsdWVuY2UvZGlzcGxheS9IaXZlL0xhbmd1YWdlTWFudWFsK1NlbGVjdCNMYW5ndWFnZU1hbnVhbFNlbGVjdC1SRUdFWENvbHVtblNwZWNpZmljYXRpb24=" title="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select#LanguageManualSelect-REGEXColumnSpecification">Hive语言手册<i class="fa fa-external-link"></i></span>，可以使用正则表达式来解决这个问题。</p><p><code>SELECT</code>语句可以在0.13.0之前的Hive版本中使用基于正则表达式的列规范，如果配置如下属性，则可以在0.13.0及更高版本中使用</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.support.quoted.identifiers=<span class="keyword">none</span></span><br></pre></td></tr></table></figure><p>Hive正则表达式使用Java正则语法，可以使用<span class="exturl" data-url="aHR0cDovL3d3dy5yZWdleHBsYW5ldC5jb20vYWR2YW5jZWQvamF2YS9pbmRleC5odG1s5p2l5rWL6K+V5q2j5YiZ5q2j56Gu5oCn44CC" title="http://www.regexplanet.com/advanced/java/index.html来测试正则正确性。">http://www.regexplanet.com/advanced/java/index.html来测试正则正确性。<i class="fa fa-external-link"></i></span></p><p>以下查询选择除ds和hr之外的所有列：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="string">`(ds|hr)?+.+`</span> <span class="keyword">FROM</span> sales</span><br></pre></td></tr></table></figure><h3 id="解决结果"><a href="#解决结果" class="headerlink" title="解决结果"></a>解决结果</h3><p>已解决。</p>]]></content>
      
      
      <categories>
          
          <category> 实用技巧 </category>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive解决使用SerDe注释异常</title>
      <link href="/2019/09/09/Hive%E4%BD%BF%E7%94%A8SerDe%E6%B3%A8%E9%87%8A%E5%8F%98%E4%B8%BAfrom%20deserializer/"/>
      <url>/2019/09/09/Hive%E4%BD%BF%E7%94%A8SerDe%E6%B3%A8%E9%87%8A%E5%8F%98%E4%B8%BAfrom%20deserializer/</url>
      
        <content type="html"><![CDATA[<center>在使用多分割符的过程中，Hive的注释中文注释显示成了from deserializer，查看发现是Bug，只能临时解决。<a id="more"></a><h4 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h4><p>当使用非内置SerDe时，添加了注释的话，字段注释会显示成<code>from deserializer</code></p><h4 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h4><p>这个问题官方目前尚未解决，但是可以设置Hive如下属性来能够正确识别注释，即使用你指定的SerDe来解析Schema——</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.serdes.using.metastore.for.schema=org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe;</span><br></pre></td></tr></table></figure><blockquote><p>参考：</p><p>Hive column comments disappearing/being replaced by “from deserializer” <span class="exturl" data-url="aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9ISVZFLTE1Mzc0" title="https://issues.apache.org/jira/browse/HIVE-15374">HIVE-15374<i class="fa fa-external-link"></i></span></p><p>Hive table always set column comment is “from deserializer” <span class="exturl" data-url="aHR0cHM6Ly9zdGFja292ZXJmbG93LmNvbS9xdWVzdGlvbnMvNDgwNTYxODcvaGl2ZS10YWJsZS1hbHdheXMtc2V0LWNvbHVtbi1jb21tZW50LWlzLWZyb20tZGVzZXJpYWxpemVy" title="https://stackoverflow.com/questions/48056187/hive-table-always-set-column-comment-is-from-deserializer">STACKOVERFLOW<i class="fa fa-external-link"></i></span></p></blockquote><h4 id="解决结果"><a href="#解决结果" class="headerlink" title="解决结果"></a>解决结果</h4><p>已解决。</p></center>]]></content>
      
      
      <categories>
          
          <category> 问题处理 </category>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> Serde </tag>
            
            <tag> 注释 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive依赖问题</title>
      <link href="/2019/09/09/Hive%E6%B7%BB%E5%8A%A0%E4%BE%9D%E8%B5%96/"/>
      <url>/2019/09/09/Hive%E6%B7%BB%E5%8A%A0%E4%BE%9D%E8%B5%96/</url>
      
        <content type="html"><![CDATA[<center>在Hive使用过程中，往往需要添加额外的依赖，或者是自定义的UDFs，这里介绍添加依赖的几种方式。<a id="more"></a>Hive的客户端分为`hive-cli`和`beeline`，现在官方推荐使用的是`beeline`客户端，其使用`hiveserver2`服务作为服务端，提供统一jdbc接口，可以在任意安装`beeline`的地方运行。先说`hive-cli`的依赖。<!-- more --><h3 id="Hive-cli依赖添加"><a href="#Hive-cli依赖添加" class="headerlink" title="Hive-cli依赖添加"></a>Hive-cli依赖添加</h3><h4 id="对单个用户"><a href="#对单个用户" class="headerlink" title="对单个用户"></a>对单个用户</h4><p><strong>临时生效</strong></p><p>登录<code>hive-cli</code>后直接使用<code>add jar</code>命令添加本地目录中的依赖，多个地址使用<code>,</code>分开，仅对当前session有效，退出后失效。这个方式支持添加放置在hdfs文件。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; add jar /opt/test.jar</span><br><span class="line">hive&gt; add jar hdfs:///tmp/test.jar</span><br></pre></td></tr></table></figure><p>使用<code>list jars</code>命令显示当前添加的依赖，使用<code>delete jar</code>删除添加的依赖。</p><p>但是这个存在的问题就是临时的，特别是当你添加一个UDF的jar包并注册了UDF的时候，当你重新进入，如果不重新添加就会报类找不到异常。</p><p><strong>永久生效</strong></p><p>一种永久生效的方法就是将<code>add jar</code>语句添加到<code>~/.hiverc</code>，这个文件可以对当前用户登录hive时先运行，里面可以设置一些hive的预设参数命令，比如<code>set hive.cli.print.header=true;</code>，或者上面的<code>add jar</code>命令，每个命令为一行，以分号结尾，如果不存在，请手动创建这个文件。</p><h4 id="对所有用户"><a href="#对所有用户" class="headerlink" title="对所有用户"></a>对所有用户</h4><p><strong>方法一</strong></p><p>将jar包拷贝到<code>${HIVE_HOME}/lib</code>中，退出Hive-cli重新进入，就可以加载到依赖中了。</p><p><strong>方法二</strong></p><p>和方法一类似，但是将依赖独立放置。在<code>${HIVE_HOME}</code>下建立名为<code>auxlib</code>的目录，将jar文件拷贝到这个目录中，退出Hive-cli重新进入，就可以加载到依赖中了。</p><p><strong>方法三</strong></p><p>修改<code>hive-env.sh</code>，新增<code>HIVE.AUX.JARS.PATH</code>环境变量，即在文件最后加入下面的一行，这里可以配置本地的<font color="red">jar文件</font>或<font color="red">放置jar的文件夹</font>，多个地址使用<code>,</code>分开，<font color="red">不支持HDFS</font>，当然，直接linux命令行里导入环境变量也行，然后重新进入Hive-cli即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_AUX_JARS_PATH=/path/jars</span><br></pre></td></tr></table></figure><p>不过如果使用的是Ambari安装的Hive，其帮助管理了这个配置项，因此在Ambari上重启Hive后配置会被还原，所以应该在Ambari中来配置这个变量，这样也能同时将配置同步到所有机器，<strong>因此如果想在每台机器上运行<code>hive-cli</code>，也必须保证相应机器上存在这个配置的目录和文件才行。</strong></p><h4 id="未验证通过"><a href="#未验证通过" class="headerlink" title="未验证通过"></a>未验证通过</h4><p><strong>其一</strong></p><p>命令行使用如下命令进入Hive-cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --auxpath /path/tets.jar</span><br></pre></td></tr></table></figure><p>实际测试不能检测到添加的依赖</p><p><strong>其二</strong></p><p>修改<code>hive-site.xml</code>，新增<code>hive.aux.jars.path</code>配置，如下所示</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.aux.jars.path<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/path/test.jar<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置后重启hive依然无法在Hive-cli检测到依赖。</p><p><strong>其三</strong></p><p>命令行使用如下命令进如Hive-cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --hiveconf hive.aux.jars.path=/path/test.jar</span><br></pre></td></tr></table></figure><h3 id="HiveServer2依赖添加"><a href="#HiveServer2依赖添加" class="headerlink" title="HiveServer2依赖添加"></a>HiveServer2依赖添加</h3><p>HiveServer2的依赖添加方式和Hive-cli的有一些相同，但是对于Hive-cli，在一台机器上配置的依赖，只能在当前机器上用，而如果在这台机器上装有HiveServer2，那么就相当于所有连接到HiveServer2的机器都连到了这台机器上，因此HiveServer2添加依赖，仅需在安装HiveServer2的机器上配置即可，这也是官方推荐使用HiveServer2的原因之一了。</p><p>另外HiveServer2并不会加载<code>${HIVE_HOME}/lib</code>中所有包，而仅有其中所需的官方包，比如其中的<code>hive-contrib-xxxx.jar</code>，其中包含了一些功能的实现，比如Hive不支持自增列、多分隔符，这个包里面就有实现方法。这就意味着你在Hive-cli中是能够使用这些功能创建表，但是HiveServer2里面会找不到这个包报错。</p><h4 id="对单个用户-1"><a href="#对单个用户-1" class="headerlink" title="对单个用户"></a>对单个用户</h4><p><strong>临时生效</strong></p><p>当然和Hive-cli中一样可以使用<code>add jar</code>命令，但是需要注意，如果这里使用本地目录，对应的本地目录是HiveServer2对应的本地目录，而不是登录的beeline客户端的本地目录，所以为了方便，这里使用hdfs目录更好，方便所有地方都可以获取到。</p><p><strong>永久生效</strong></p><p>如果对应<code>.hiverc</code>文件实现的功能，beeline可以使用<code>-i</code>来实现进入客户端前的初始化，比如将<code>add jar</code>命令写入文件<code>/tmp/beeline-init</code>文件中，然后使用如下命令进入客户端，最后的<code>-n</code>是使用hive用户的意思，可以不要：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline -i /tmp/beeline-init -u jdbc:hive2://hadoop1:10000 -n hive</span><br></pre></td></tr></table></figure><p>但是只是将每次需要手动输入的添加jar包的命令变成进入前先自动进行，如果使用的是本地目录，对应的是HiveServer2服务器上的目录。</p><h4 id="对所有用户-1"><a href="#对所有用户-1" class="headerlink" title="对所有用户"></a>对所有用户</h4><p><strong>方法一</strong></p><p>因为HiveServer2不会加载<code>${HIVE_HOME}/lib</code>下其他包，所以并不能通过把包放到其中，然后重启HiveServer2服务生效，但是可以放置到自建的<code>auxlib</code>中。</p><p>同Hive-cli中的方法二，在<code>${HIVE_HOME}</code>下建立名为<code>auxlib</code>的目录，将jar文件拷贝到这个目录中，然后重启HiveServer2服务，这时在使用beeline连接时即可检测到依赖了，而且在HiveServer2运行中，如果删除这个目录下的依赖，仍然能够正常使用，不过重启服务后就不能使用了。这也说明HiveServer2只在启动时加载其中的文件，运行在就不需要这个目录了，当重启后会重新加载这个目录中的文件。</p><p><strong>方法二</strong></p><p>同Hive-cli方法三，在HiveServer2服务端，修改<code>hive-env.sh</code>，新增<code>HIVE.AUX.JARS.PATH</code>环境变量，即在文件最后加入下面的一行，这里可以配置本地的<font color="red">jar文件</font>或<font color="red">放置jar的文件夹</font>，多个地址使用<code>,</code>分开，<font color="red">不支持HDFS</font>，当然，直接linux命令行里导入环境变量也行，然后重启HiveServer2服务即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_AUX_JARS_PATH=/path/jars</span><br></pre></td></tr></table></figure><p>不过如果使用的是Ambari安装的Hive，其帮助管理了这个配置项，因此在Ambari上重启Hive后配置会被还原，所以应该在Ambari中来配置这个变量。</p><h4 id="未验证通过-1"><a href="#未验证通过-1" class="headerlink" title="未验证通过"></a>未验证通过</h4><p>同Hive-cli，其中三种方式均为验证通过。</p></center>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> HiveServer2 </tag>
            
            <tag> 依赖 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zeppelin取消账户密码</title>
      <link href="/2019/09/09/Zeppelin%E5%8F%96%E6%B6%88%E8%B4%A6%E6%88%B7%E5%AF%86%E7%A0%81/"/>
      <url>/2019/09/09/Zeppelin%E5%8F%96%E6%B6%88%E8%B4%A6%E6%88%B7%E5%AF%86%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<center>Ambari安装Zeppelin默认启动了账户密码登录，默认为admin:admin，为方便使用，关闭账户密码。<a id="more"></a><h3 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h3><p>使用Ambari安装的Zeppelin自动配置了账号密码登录，这样比较安全，默认账户密码是admin:admin，但是为了方便起见，想要不登录直接编辑可以吗？那是自然！只要开启匿名登录即可。</p><p><img src="//heltman.github.io/2019/09/09/Zeppelin取消账户密码/1567682528350.png" alt="1567682528350"></p><h3 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h3><p>过程较为简单，主要分为以下三步。</p><h4 id="启用匿名登录"><a href="#启用匿名登录" class="headerlink" title="启用匿名登录"></a>启用匿名登录</h4><p>Ambari界面可以直接在<code>Advanced zeppelin-config</code>中修改<code>zeppelin.anonymous.allowed</code>为<code>true</code>即可（原本为<code>false</code>）.</p><p><img src="//heltman.github.io/2019/09/09/Zeppelin取消账户密码/1567682019709.png" alt="1567682019709"></p><h4 id="关闭验证登录"><a href="#关闭验证登录" class="headerlink" title="关闭验证登录"></a>关闭验证登录</h4><p>Ambari界面直接在<code>Advanced zeppelin-shiro.ini</code>中修改<code>shiro_ini_content</code>的最后，打开<code>/** = anon</code>的注释，注释掉<code>/** = authc</code>，改完如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[urls]</span><br><span class="line"># This section is used for url-based security.</span><br><span class="line"># You can secure interpreter, configuration and credential information by urls. Comment or uncomment the below urls that you want to hide.</span><br><span class="line"># anon means the access is anonymous.</span><br><span class="line"># authc means Form based Auth Security</span><br><span class="line"># To enfore security, comment the line below and uncomment the next one</span><br><span class="line">/api/version = anon</span><br><span class="line">#/api/interpreter/** = authc, roles[admin]</span><br><span class="line">#/api/configurations/** = authc, roles[admin]</span><br><span class="line">#/api/credential/** = authc, roles[admin]</span><br><span class="line">/** = anon</span><br><span class="line">#/** = authc</span><br></pre></td></tr></table></figure><h4 id="重启服务生效"><a href="#重启服务生效" class="headerlink" title="重启服务生效"></a>重启服务生效</h4><p>保存后按照要求重启服务即可</p><h3 id="解决结果"><a href="#解决结果" class="headerlink" title="解决结果"></a>解决结果</h3><p><img src="//heltman.github.io/2019/09/09/Zeppelin取消账户密码/1567683023745.png" alt="1567683023745"></p></center>]]></content>
      
      
      <categories>
          
          <category> 问题处理 </category>
          
          <category> Zeppelin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zeppelin </tag>
            
            <tag> 账户密码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zeppelin问题处理</title>
      <link href="/2019/09/05/Zeppelin%E5%90%8C%E6%97%B6%E9%85%8D%E7%BD%AESpark%E5%92%8CSpark2/"/>
      <url>/2019/09/05/Zeppelin%E5%90%8C%E6%97%B6%E9%85%8D%E7%BD%AESpark%E5%92%8CSpark2/</url>
      
        <content type="html"><![CDATA[<center>使用Ambari同时安装Spark1和Spark2，再安装Zeppelin之后发现2个Spark执行引擎都用不了。抱着解决问题的态度，仔细查看错误日志找找原因吧。</center><a id="more"></a><blockquote><p>起因：准备构建全新大数据平台，因此顺便研究一下将Spark1升级到Spark2，同时也就研究一下Zeppelin来可视化分析。现有平台是<code>HDP2.6.5</code>，自带<code>Spark1.6.3</code>、<code>Spark2.3.0</code>和<code>Zeppelin0.7.3</code>，使用Ambari轻松装好。</p><p><strong>可是问题来了，Zeppelin中Spark处理引擎都不能使用？？？</strong></p><p>难道打开方式不对？去Hortonworks官方文档里面看了一下，根本没提这回事，只说了同时安装Spark1和2时如何切换版本，大概这部分没有测试？</p></blockquote><h3 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h3><p>同时安装Spark1和2时，Zeppelin的Spark执行引擎都不能用，Web上执行后直接报错，报错大多为空指针异常、连接拒绝以及方法或类找不到等：</p><ul><li><p>空指针异常</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">at org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)</span><br><span class="line">at org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:33)</span><br><span class="line">at org.apache.zeppelin.spark.SparkInterpreter.createSparkContext_2(SparkInterpreter.java:348)</span><br><span class="line">at org.apache.zeppelin.spark.SparkInterpreter.createSparkContext(SparkInterpreter.java:337)</span><br><span class="line">at org.apache.zeppelin.spark.SparkInterpreter.getSparkContext(SparkInterpreter.java:142)</span><br></pre></td></tr></table></figure></li><li><p>连接拒绝</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: </span><br><span class="line">at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)</span><br><span class="line">at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)</span><br><span class="line">at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)</span><br><span class="line">at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:201)</span><br><span class="line">at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:65)</span><br><span class="line">at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:64)</span><br><span class="line">at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)</span><br><span class="line">... 4 more</span><br><span class="line">Caused by: java.io.IOException: Connection from alone/192.168.128.132:34711 closed</span><br></pre></td></tr></table></figure></li><li><p>方法找不到</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.NoSuchMethodError: org.apache.spark.network.util.AbstractFileRegion.transferred()J</span><br><span class="line">at org.apache.spark.network.util.AbstractFileRegion.transfered(AbstractFileRegion.java:28)</span><br><span class="line">at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:228)</span><br><span class="line">at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:282)</span><br><span class="line">at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:879)</span><br></pre></td></tr></table></figure></li></ul><h3 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h3><h4 id="相关说明"><a href="#相关说明" class="headerlink" title="相关说明"></a>相关说明</h4><p>首先，经过官方说明，如果想使用多个Spark，<font color="red">不能做的操作</font>就是直接修改Zeppelin的配置文件<code>zeppelin-env.sh</code>，向其中添加<code>SPARK_HOME</code>环境变量，而Ambari中则是对应向<code>Advanced zeppelin-env</code>中<code>zeppelin_env_content</code>添加环境变量，这样做的话就只能使用一个Spark了。因此如果已经有这个配置的话，需要将配置中<code>SPARK_HOME</code>这部分注释掉或者删除。</p><p>当然，如果只有一个Spark，可以在这里直接配置固定的即可。</p><h4 id="解决Spark1错误"><a href="#解决Spark1错误" class="headerlink" title="解决Spark1错误"></a>解决Spark1错误</h4><h5 id="查看错误日志"><a href="#查看错误日志" class="headerlink" title="查看错误日志"></a>查看错误日志</h5><p>Ambari安装的Zeppelin的日志存放位置在<code>/var/log/zeppelin</code>，打开路径就可以看到对应执行引擎的日志，Spark1引擎对应的日志文件名为<code>zeppelin-interpreter-spark-spark-zeppelin-${hostname}.log</code>，一系列错误如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: scala.reflect.api.JavaUniverse.runtimeMirror(Ljava/lang/ClassLoader;)Lscala/reflect/api/JavaMirrors$JavaMirror;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: error reading Scala signature of scala.package: scala.collection.immutable.Range.vali</span><br><span class="line">dateRangeBoundaries(Lscala/Function1;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.AbstractMethodError: org.apache.spark.network.protocol.MessageWithHeader.touch(Ljava/lang/Object;)Lio/netty/util/ReferenceCounted;</span><br><span class="line">at io.netty.util.ReferenceCountUtil.touch(ReferenceCountUtil.java:77)</span><br><span class="line">at io.netty.channel.DefaultChannelPipeline.touch(DefaultChannelPipeline.java:116)</span><br></pre></td></tr></table></figure><h5 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h5><p>因为没有在zeppelin-env中配置<code>SPARK_HOME</code>环境变量，Zeppelin找不到依赖，可是官方说明在执行引擎中配置就可以了啊？检查执行引擎确实也已经设置了？</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin同时配置Spark和Spark2/1567675888659.png" alt="1567675888659"></p><p>分析看来其中只找到了bin目录下的执行脚本，但是没有找到依赖，这是为什么呢？</p><p>zeppelin的执行引擎是分组的，spark和spark2的分组都是spark，而这个分组则对应着zeppelin安装目录下的interpreter目录中的一个文件夹。</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin同时配置Spark和Spark2/1567676458882.png" alt="1567676458882"></p><p><img src="//heltman.github.io/2019/09/05/Zeppelin同时配置Spark和Spark2/1567676550472.png" alt="1567676550472"></p><p>这个文件夹中会有对应执行引擎的jar包和相关依赖，查看<code>spark</code>文件夹中就会发现名为<code>zeppelin-spark_2.11-0.7.3.2.6.5.0-292.jar</code>的执行引擎和<code>dep</code>文件夹。</p><p>里面包含一个200多MB的依赖文件<code>zeppelin-spark-dependencies_2.11-0.7.3.2.6.5.0-292.jar</code>，解压打开文件发现其根目录<code>spark-version-info.properties</code>文件中指明了使用的Spark版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">version=2.3.0.2.6.5.0-292</span><br><span class="line">user=jenkins</span><br><span class="line">revision=b6b578c8dff89bfc07981ea3bda074262c3800ad</span><br><span class="line">branch=HEAD</span><br><span class="line">date=2018-05-11T07:55:12Z</span><br><span class="line">url=git@github.com:hortonworks/spark2.git</span><br></pre></td></tr></table></figure><p>那么就说明这个依赖包是针对Spark2的，因此属于这个组的Spark1如果没有在环境变量里指明<code>SPARK_HOME</code>，直接使用的话依赖确实可能有问题。</p><h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><p>直接将spark1的依赖<code>/usr/hdp/current/spark-client/lib/spark-assembly-1.6.3.2.6.5.0-292-hadoop2.7.3.2.6.5.0-292.jar</code>拷贝到这个目录<code>/usr/hdp/current/zeppelin/interpreter/spark/dep</code>，这样能够解决spark1报错的问题，但是因为修改了组模板，可能存在和Spark2的依赖冲突的情况，因此推荐的解决方法是修改Zeppelin中Spark1对应执行引擎的依赖，点击进入编辑：</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin同时配置Spark和Spark2/1567679649778.png" alt="1567679649778"></p><p>拉到最下方添加依赖：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/hdp/current/spark-client/lib/spark-assembly-1.6.3.2.6.5.0-292-hadoop2.7.3.2.6.5.0-292.jar</span><br></pre></td></tr></table></figure><p>或者如图中的<code>/usr/hdp/2.6.5.0-292/spark/lib/spark-hdp-assembly.jar</code>，因为<code>current/spark-client</code>是软链到<code>2.6.5.0-292/spark</code>，而<code>spark-hdp-assembly.jar</code>只是<code>spark-assembly-1.6.3.2.6.5.0-292-hadoop2.7.3.2.6.5.0-292.jar</code>的软连接而已，随便用哪个。</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin同时配置Spark和Spark2/1567679738798.png" alt="1567679738798"></p><p>点击保存后重启该执行引擎（上面编辑键右边一个），至此解决完毕。</p><h4 id="解决Spark2错误"><a href="#解决Spark2错误" class="headerlink" title="解决Spark2错误"></a>解决Spark2错误</h4><h5 id="查看错误日志-1"><a href="#查看错误日志-1" class="headerlink" title="查看错误日志"></a>查看错误日志</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Caused by: io.netty.channel.socket.ChannelOutputShutdownException: Channel output shutdown</span><br><span class="line">at io.netty.channel.AbstractChannel$AbstractUnsafe.shutdownOutput(AbstractChannel.java:587)</span><br><span class="line">... 22 more</span><br><span class="line">Caused by: java.lang.NoSuchMethodError: org.apache.spark.network.util.AbstractFileRegion.transferred()J</span><br><span class="line">at org.apache.spark.network.util.AbstractFileRegion.transfered(AbstractFileRegion.java:28)</span><br><span class="line">at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:228)</span><br><span class="line">at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:282)</span><br><span class="line">at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:879)</span><br><span class="line"></span><br><span class="line">Exit code: 13</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MultiException[java.lang.NoClassDefFoundError: com/fasterxml/jackson/jaxrs/json/JacksonJaxbJsonProvider, java.lang.NoClassDefFoundError: com/fasterxml/jackson/jaxrs/json/JacksonJaxbJsonProvider]</span><br><span class="line"></span><br><span class="line">Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: </span><br><span class="line">at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)</span><br><span class="line">at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)</span><br><span class="line">at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)</span><br><span class="line">at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:201)</span><br><span class="line">at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:65)</span><br><span class="line">at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:64)</span><br><span class="line">at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)</span><br><span class="line">... 4 more</span><br><span class="line">Caused by: java.io.IOException: Connection from alone/192.168.128.132:34711 closed</span><br><span class="line"></span><br><span class="line">Exit code: 1</span><br></pre></td></tr></table></figure><h5 id="问题分析-1"><a href="#问题分析-1" class="headerlink" title="问题分析"></a>问题分析</h5><p>根据Spark1中的分析，Spark2显然不是相同的问题，当然错误日志也可以看得很明显，前一个找不到netty的某一个方法，后一个是找不到jackson的类，因此是依赖有问题。</p><p>检查Zeppelin的依赖目录<code>/usr/hdp/current/zeppelin/lib</code>，其中netty版本为<code>4.0.52.Final</code>，jackson版本为<code>2.5.4</code></p><p>检查Spark2的依赖目录<code>/usr/hdp/current/spark2-client/jars</code>，其中netty版本为<code>3.9.9.Final</code>和<code>4.1.17.Final</code>，jackson版本为<code>2.6.7</code></p><p>好了，知道怎么做了！</p><h5 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h5><p>知道原因，解决就很简单了，只需要将Zeppelin中对应依赖删除换成Spark的即可。</p><p>删除Zeppelin的lib中<code>netty-all-4.0.52.Final.jar</code>，将spark2中jars下<code>netty-3.9.9.Final.jar</code>，<code>netty-all-4.1.17.Final.jar</code>移到目录下</p><p>删除Zeppelin的lib中<code>jackson-annotations-2.5.4.jar</code>、<code>jackson-core-2.5.4.jar</code>、<code>jackson-databind-2.5.4.jar</code>，将spark2中jars下<code>jackson-annotations-2.6.7.jar</code>、<code>jackson-core-2.6.7.jar</code>、<code>jackson-databind-2.6.7.1.jar</code></p><p>之后请<font color="red">重启Zeppelin服务</font>来重新加载依赖</p><h3 id="解决结果"><a href="#解决结果" class="headerlink" title="解决结果"></a>解决结果</h3><p>解决后两个引擎都能正常运行，完美！</p><p><img src="//heltman.github.io/2019/09/05/Zeppelin同时配置Spark和Spark2/1567681494806.png" alt="1567681494806"></p>]]></content>
      
      
      <categories>
          
          <category> 问题处理 </category>
          
          <category> Zeppelin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zeppelin </tag>
            
            <tag> Spark </tag>
            
            <tag> 问题处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive中文注释乱码</title>
      <link href="/2019/09/04/Hive%E8%A7%A3%E5%86%B3%E4%B8%AD%E6%96%87%E6%B3%A8%E9%87%8A%E4%B9%B1%E7%A0%81/"/>
      <url>/2019/09/04/Hive%E8%A7%A3%E5%86%B3%E4%B8%AD%E6%96%87%E6%B3%A8%E9%87%8A%E4%B9%B1%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<center>Hive安装前已经将MySQL设置了默认UTF-8编码，但是还是存在中文注释乱码的问题，查看发现Hive元数据表还是用的latin，无语，只能再改一下了。</center><a id="more"></a><h4 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h4><p>创建表的时候，如果添加了comment字段且包含中文，创建后查询时为乱码。</p><h4 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h4><p>登录保存Hive元数据的数据库，修改<code>表</code>、<code>分区</code>、<code>视图</code>的编码为<code>utf8</code>即可，以下以MySQL为例——</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> hive;</span><br><span class="line"><span class="comment">--修改表字段注解和表注解</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> COLUMNS_V2 <span class="keyword">modify</span> <span class="keyword">column</span> <span class="keyword">COMMENT</span> <span class="built_in">varchar</span>(<span class="number">256</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> TABLE_PARAMS <span class="keyword">modify</span> <span class="keyword">column</span> PARAM_VALUE <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="comment">--修改分区字段注解</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_PARAMS <span class="keyword">modify</span> <span class="keyword">column</span> PARAM_VALUE <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_KEYS <span class="keyword">modify</span> <span class="keyword">column</span> PKEY_COMMENT <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="comment">--修改索引注解</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> INDEX_PARAMS <span class="keyword">modify</span> <span class="keyword">column</span> PARAM_VALUE <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure><blockquote><p>注意：已经创建的表还会是乱码，请删除后重建，注意内部表删除会删除数据！</p></blockquote><h4 id="解决结果"><a href="#解决结果" class="headerlink" title="解决结果"></a>解决结果</h4><p>已解决。</p>]]></content>
      
      
      <categories>
          
          <category> 问题处理 </category>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> 注释 </tag>
            
            <tag> 乱码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ambari插件问题处理</title>
      <link href="/2019/09/02/Ambari%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/"/>
      <url>/2019/09/02/Ambari%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<center>使用Ambari安装完HDP后，其中的插件使用或多或少存在一些问题，以下收集了一些解决方案供参考。</center><a id="more"></a><h3 id="Files-View插件"><a href="#Files-View插件" class="headerlink" title="Files View插件"></a>Files View插件</h3><p><strong>问题：</strong>进行操作时弹出权限拒绝</p><p><strong>解决方法：</strong></p><p>在Ambari界面修改HDFS的配置中Custom core-site，新增：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop.proxyuser.admin.groups=*</span><br><span class="line">hadoop.proxyuser.admin.hosts=*</span><br></pre></td></tr></table></figure><p>其中admin为报没有权限的用户名</p><p>如果配置后仍然报没有权限，说明部署Ambari-server的机器上没有该用户，需要登录机器新增该用户，并添加到users、hdfs和hadoop组：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">useradd -G hdfs admin </span><br><span class="line">usermod -a -G users admin </span><br><span class="line">usermod -a -G hadoop admin</span><br></pre></td></tr></table></figure><p>现在应该有权限进行操作了</p><h3 id="Hive-View和Hive-View-2-0插件"><a href="#Hive-View和Hive-View-2-0插件" class="headerlink" title="Hive View和Hive View 2.0插件"></a>Hive View和Hive View 2.0插件</h3><p><strong>问题：</strong>数据存储异常，主要报错如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.ambari.view.PersistenceException: Caught exception trying to store view entity JobImpl&#123;id='null, owner='admin, hiveQueryId='null, dagId='null, queryId='null&#125;</span><br><span class="line">......</span><br><span class="line">Caused by: javax.persistence.RollbackException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException</span><br><span class="line">Internal Exception: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'ambari.ds_jobimpl_5' doesn't exist</span><br><span class="line">Error Code: 1146</span><br><span class="line">Call: INSERT INTO DS_JOBIMPL_5 (DS_id, DS_applicationId, DS_confFile, DS_dagId, DS_dagName, DS_dataBase, DS_dateSubmitted, DS_duration, DS_forcedContent, DS_globalSettings, DS_guid, DS_hiveQueryId, DS_logFile, DS_owner, DS_queryFile, DS_queryId, DS_referrer, DS_sessionTag, DS_sqlState, DS_status, DS_statusDir, DS_statusMessage, DS_title) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)</span><br></pre></td></tr></table></figure><p><strong>解决方法：</strong>需要到Mysql的ambari数据库中建立相应表，建表语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> DS_JOBIMPL_5 (</span><br><span class="line">ds_id <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">ds_applicationid <span class="built_in">TEXT</span>,</span><br><span class="line">ds_conffile <span class="built_in">TEXT</span>,</span><br><span class="line">ds_dagid <span class="built_in">TEXT</span>,</span><br><span class="line">ds_dagname <span class="built_in">TEXT</span>,</span><br><span class="line">ds_database <span class="built_in">TEXT</span>,</span><br><span class="line">ds_datesubmitted <span class="built_in">BIGINT</span>,</span><br><span class="line">ds_duration <span class="built_in">BIGINT</span>,</span><br><span class="line">ds_forcedcontent <span class="built_in">TEXT</span>,</span><br><span class="line">ds_globalsettings <span class="built_in">TEXT</span>,</span><br><span class="line">DS_guid <span class="built_in">TEXT</span>,</span><br><span class="line">DS_hiveQueryId <span class="built_in">TEXT</span>,</span><br><span class="line">ds_logfile <span class="built_in">TEXT</span>,</span><br><span class="line">ds_owner <span class="built_in">TEXT</span>,</span><br><span class="line">ds_queryfile <span class="built_in">TEXT</span>,</span><br><span class="line">ds_queryid <span class="built_in">TEXT</span>,</span><br><span class="line">ds_referrer <span class="built_in">TEXT</span>,</span><br><span class="line">ds_sessiontag <span class="built_in">TEXT</span>,</span><br><span class="line">ds_sqlstate <span class="built_in">TEXT</span>,</span><br><span class="line">ds_status <span class="built_in">TEXT</span>,</span><br><span class="line">ds_statusdir <span class="built_in">TEXT</span>,</span><br><span class="line">ds_statusmessage <span class="built_in">TEXT</span>,</span><br><span class="line">ds_title <span class="built_in">TEXT</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ds_id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 问题处理 </category>
          
          <category> Ambari </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ambari </tag>
            
            <tag> 插件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库架构简单介绍</title>
      <link href="/2019/08/28/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/"/>
      <url>/2019/08/28/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<center>针对数据仓库架构的相关概念进行简单的解释说明，对数据仓库的建模方式进行简单的描述。</center><a id="more"></a><p><img src="//heltman.github.io/2019/08/28/数据仓库架构/%E5%A4%A7%E6%95%B0%E6%8D%AE.jpg" alt="大数据"></p><h2 id="数据仓库概念"><a href="#数据仓库概念" class="headerlink" title="数据仓库概念"></a>数据仓库概念</h2><h3 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h3><h4 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h4><ul><li>需求的变化</li><li>业务系统的建设逐渐完善</li><li>分析类需求不断增加</li><li>不断增加的信息孤岛导致数据集成问题不断增加</li></ul><h4 id="技术发展状况"><a href="#技术发展状况" class="headerlink" title="技术发展状况"></a>技术发展状况</h4><ul><li>关系数据库技术日趋成熟</li><li>报表和复杂查询处理起来非常困难</li><li>各个系统之间数据不一致</li></ul><p>常用关系型数据仓库：greenplum，vertica</p><h4 id="ODS：操作型数据仓库"><a href="#ODS：操作型数据仓库" class="headerlink" title="ODS：操作型数据仓库"></a>ODS：操作型数据仓库</h4><p>早期数据仓库模型，为企业提供即时的，操作型的，继承的数据集合，具有面向主题性，集成性，动态性，即时性，明细性等特性。面向即时的，一般是一小时，到一分钟。存储时间较短，7天5天等。基于战术的操作，和基于战略的数据仓库有所不同。</p><h4 id="STAGING-AREA（缓存区，分段存储区）"><a href="#STAGING-AREA（缓存区，分段存储区）" class="headerlink" title="STAGING AREA（缓存区，分段存储区）"></a>STAGING AREA（缓存区，分段存储区）</h4><p>为了保证数据移动的顺利进行而开设的阶段性数据存储空间，和ODS有重合，有ODS可以不要这个。如果数据仓库数据有问题可以直接从缓存区拿取数据，不需要从业务处拿取。</p><h4 id="DATA-MART（数据集市）"><a href="#DATA-MART（数据集市）" class="headerlink" title="DATA MART（数据集市）"></a>DATA MART（数据集市）</h4><p>数据集市是完整的数据仓库的一个逻辑子集，为了特定的应用目的或应用范围，而从数据仓库独立出来的一部分数据。，也可称为部门数据或主题数据。</p><h4 id="数据仓库设计"><a href="#数据仓库设计" class="headerlink" title="数据仓库设计"></a>数据仓库设计</h4><blockquote><p><strong>源系统 –&gt; ODS –&gt; DW –&gt; DM</strong></p></blockquote><p>建立数据仓库与数据集市，一般采用“自顶向下”和“自下而上”相结合的设计思想。</p><h4 id="OLAP（在线分析处理）"><a href="#OLAP（在线分析处理）" class="headerlink" title="OLAP（在线分析处理）"></a>OLAP（在线分析处理）</h4><p>OLAP是一种多为分析技术，用来满足决策用户在大量的业务数据中，从多角度探索业务活动的规律性、市场的运作趋势的分析需求，并辅助他们进行战略决策的制定。OLAP系统按照存储格式可以分为关系OLAP（ROLAP）、多维OLAP（MOLAP）和混合型OLAP（HOLAP）三种类型。Kylin属于多维OLAP</p><h4 id="OLAP-VA-OLTP（在线分析系统和在线交易系统）"><a href="#OLAP-VA-OLTP（在线分析系统和在线交易系统）" class="headerlink" title="OLAP VA OLTP（在线分析系统和在线交易系统）"></a>OLAP VA OLTP（在线分析系统和在线交易系统）</h4><table><thead><tr><th>ITEM</th><th>OLTP</th><th>OLAP</th></tr></thead><tbody><tr><td>用户</td><td>操作人员</td><td>分析决策人员，高级管理人员</td></tr><tr><td>功能</td><td>日常操作处理</td><td>分析决策</td></tr><tr><td>设计</td><td>面向应用</td><td>面向主题</td></tr><tr><td>数据</td><td>当前的，最新的细节的，二维的，分立的</td><td>历史的，聚集的，多维的，集成的，统一的</td></tr><tr><td>存取</td><td>读/写数十条记录</td><td>读上百万条记录</td></tr><tr><td>工作单位</td><td>简单的读写</td><td>复杂查询</td></tr><tr><td>DB大小</td><td>GB到TB</td><td>TB-PB级别</td></tr><tr><td>度量</td><td>事物吞吐量</td><td>查询吞吐量、响应时间</td></tr></tbody></table><hr><h2 id="数据仓库建模"><a href="#数据仓库建模" class="headerlink" title="数据仓库建模"></a>数据仓库建模</h2><p>范式建模、维度建模、Data Valut、Anchor</p><h3 id="范式建模：ER模型"><a href="#范式建模：ER模型" class="headerlink" title="范式建模：ER模型"></a>范式建模：ER模型</h3><p>自上而下（EDW-DM）的数据仓库架构。操作型或事务型系统的数据源，通过ETL抽取转换和加载到数据仓库的ODS层，然后通过ODS的数据建设原子数据的数据仓库EDW，EDW不是多维格式的，不方便上层应用做数据分析，所以需要通过汇总建设成多维格式的数据集市层。</p><p>优势：易于维护，高度集成</p><p>劣势：结构死板，部署周期较长</p><h3 id="维度建模"><a href="#维度建模" class="headerlink" title="维度建模"></a>维度建模</h3><p>以分析决策的需求出发构建模型，构建的数据模型为分析需求服务，因此它重点解决用户如何更快速完成分析需求，同时还有叫好的大规模复杂查询的响应性能。</p><p>维度模型是由一个规范化的事实表和反规范化的一些维度组成的</p><ul><li>一种非规范化的关系模型</li><li>表和表之间的关系通过关键字和外键来定义</li></ul><p>以良好的可理解性和方便的产生报表来进行数据组织，很少考虑修改的性能</p><p>通过SQL或者相关的工具实现数据的查询和维护</p><h4 id="维度表"><a href="#维度表" class="headerlink" title="维度表"></a>维度表</h4><p>每一张维度表对应现实世界中的一个对象或者概念</p><ul><li>例如：客户、产品、日期、地区、商场</li></ul><p>维度表的特征：</p><ul><li>包含了众多描述性的列：维度表的范围很宽（具有多个属性）</li><li>通常情况下，跟事实表相比，行数相对较小：通常&lt;10万条</li><li>内容相对固定：几乎就是一个类查找表或者编码表</li></ul><h4 id="事实表"><a href="#事实表" class="headerlink" title="事实表"></a>事实表</h4><p>每一个事实表通常包含了处理所关心的度量值</p><p>每一个事实表的行包括</p><ul><li>具有可加性的数值型的度量值</li><li>与维度表相连接的外键<ul><li>通常具有两个和两个以上的外键</li><li>外键之间表示维度表之间多对多的关系</li></ul></li></ul><p>事实表的特征</p><ul><li>非常大：包含几万、几十万甚至千万的记录</li><li>内容相对的窄，列数较少</li><li>经常发生变化</li></ul><h4 id="维度模型设计过程"><a href="#维度模型设计过程" class="headerlink" title="维度模型设计过程"></a>维度模型设计过程</h4><ol><li>选择业务过程</li><li>声明粒度</li><li>确定维度</li><li>确定事实</li></ol><hr><h2 id="数据治理"><a href="#数据治理" class="headerlink" title="数据治理"></a>数据治理</h2><h3 id="元数据管理"><a href="#元数据管理" class="headerlink" title="元数据管理"></a>元数据管理</h3><ul><li>技术元数据<ul><li>表级血缘分析</li><li>字段级别血缘分析</li><li>影响分析</li><li>数据审计</li></ul></li><li>业务元数据</li></ul><h3 id="主数据管理"><a href="#主数据管理" class="headerlink" title="主数据管理"></a>主数据管理</h3><hr><h2 id="数据仓库架构"><a href="#数据仓库架构" class="headerlink" title="数据仓库架构"></a>数据仓库架构</h2><p><img src="//heltman.github.io/2019/08/28/数据仓库架构/1558625207687.png" alt="1558625207687"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
            <tag> 入门文档 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
