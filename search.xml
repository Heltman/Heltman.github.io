<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Spark简单学习说明</title>
      <link href="/2019/08/28/Spark%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/08/28/Spark%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>Spark简单解释说明，待更新……</p><a id="more"></a><h2 id="配置Spark"><a href="#配置Spark" class="headerlink" title="配置Spark"></a>配置Spark</h2><p>连接Spark到任意hadoop集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### in conf/spark-env.sh ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If 'hadoop' binary is on your PATH</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(hadoop classpath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># With explicit path to 'hadoop' binary</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/path/to/hadoop/bin/hadoop classpath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Passing a Hadoop configuration directory</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(hadoop --config /path/to/configs classpath)</span><br></pre></td></tr></table></figure><h2 id="写Spark程序"><a href="#写Spark程序" class="headerlink" title="写Spark程序"></a>写Spark程序</h2><h3 id="与Spark连接"><a href="#与Spark连接" class="headerlink" title="与Spark连接"></a>与Spark连接</h3><p>Spark 1.6.1使用Scala 2.10。要在Scala中编写应用程序，您需要使用兼容的Scala版本（例如2.10.X）。</p><h4 id="编写Spark程序所需mvn依赖"><a href="#编写Spark程序所需mvn依赖" class="headerlink" title="编写Spark程序所需mvn依赖"></a>编写Spark程序所需mvn依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果需要连接HDFS，还需要如下依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>最后需要在程序中import一些Spark类</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br></pre></td></tr></table></figure><p>（在Spark 1.3.0之前，您需要显式导入org.apache.spark.SparkContext._以启用必要的隐式转换。)</p><h3 id="初始化Spark"><a href="#初始化Spark" class="headerlink" title="初始化Spark"></a>初始化Spark</h3><p>Spark程序必须做的第一件事是创建一个SparkContext对象，它告诉Spark如何访问集群。要创建SparkContext，首先需要构建一个包含有关应用程序信息的SparkConf对象。</p><p>每个JVM只能激活一个SparkContext。在创建新的SparkContext之前，必须先<code>stop()</code>。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(appName).setMaster(master)</span><br><span class="line"><span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure><p>appName参数是应用程序在集群UI上显示的名称。 master是Spark，Mesos或YARN群集URL，或者是以本地模式运行的特殊“本地”字符串。 实际上，在群集上运行时，您不希望在程序中对master进行硬编码，而是使用spark-submit启动应用程序并在那里接收它。 但是，对于本地测试和单元测试，您可以传递“local”以在进程中运行Spark。</p><h4 id="使用shell"><a href="#使用shell" class="headerlink" title="使用shell"></a>使用shell</h4><p>在Spark shell中，已经在名为<code>sc</code>的变量中为您创建了一个特殊的SparkContext 。制作自己的SparkContext将无法正常工作。您可以使用<code>--master</code>参数设置上下文连接到的主服务器，并且可以通过将逗号分隔的列表传递给参数来将JAR添加到类路径中<code>--jars</code>。您还可以通过为参数提供以逗号分隔的maven坐标列表，将依赖项（例如Spark包）添加到shell会话中<code>--packages</code>。任何可能存在依赖关系的其他存储库（例如SonaType）都可以传递给<code>--repositories</code>参数。例如，要<code>bin/spark-shell</code>在四个核心上运行，请使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-shell --master <span class="built_in">local</span>[4]</span><br></pre></td></tr></table></figure><p>或者，要将code.jar添加到其类路径中，请使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-shell --master <span class="built_in">local</span>[4] --jars code.jar</span><br></pre></td></tr></table></figure><p>要使用maven坐标包含依赖项:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-shell --master <span class="built_in">local</span>[4] --packages <span class="string">"org.example:example:0.1"</span></span><br></pre></td></tr></table></figure><p>有关选项的完整列表，请运行<code>spark-shell --help</code>。在幕后， <code>spark-shell</code>调用更通用的<span class="exturl" data-url="aHR0cDovL3NwYXJrLmFwYWNoZS5vcmcvZG9jcy8xLjYuMS9zdWJtaXR0aW5nLWFwcGxpY2F0aW9ucy5odG1s" title="http://spark.apache.org/docs/1.6.1/submitting-applications.html"><code>spark-submit</code>脚本<i class="fa fa-external-link"></i></span>。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 入门文档 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
